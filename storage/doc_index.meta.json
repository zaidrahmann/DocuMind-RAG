[
{
"filename": "1040_paper_2.pdf",
"page_number": 1,
"chunk_index": 0,
"token_count": 512,
"text": "2026 IEEE International Conference on Electrical, Computer & Telecommunication Engineering (ICECTE 2026)\n29–31 January 2026, Rajshahi-6204, Bangladesh\nModern Imaging and Learning Methods for\nBurn-Depth Assessment: A Structured Review\nAtiya Masuda Siddika 1, Zaid Rehman 2, Md. Fahmidul Hoque 3, Mohammad Shamsul Arefin 1, Pranab Kumar Dhar 1\n1Department of Computer Science and Engineering, Chittagong University of Engineering & Technology (CUET), Chattogram, Bangladesh\n2BRAC University, Dhaka, Bangladesh\n3Jahangirnagar University, Savar, Dhaka, Bangladesh\nEmails: atiya.siddika@cuet.ac.bd 1, zaidrehman3103@gmail.com 2, fahmidulhoque19@gmail.com3, sarefin@cuet.ac.bd1, pranabdhar81@cuet.ac.bd 1\nAbstract—Burn injuries require accurate assessment to guide\nquick treatment, specially decisions on debridement and trans-\nplantation. Burn depth assessment is the one of the most critical\nburn related actions, although visual inspection is often unreliable\nbecause deeper tissue changes are not always visible when wounds\nheal. Recent work has combined different imaging modalities\nwith machine-learning and deep-learning models to improve\nconsistency and early decision-making. However, existing reviews\ndo not fully integrate the growing range of models, datasets, pre-\nprocessing methods, and evaluation practices used in burn-depth\nanalysis. This paper provides a structured taxonomy linking\nimaging modalities, preprocessing steps, and learning models. We\nconduct an experiment indicating that advanced preprocessing\ntechniques significantly increase model performance compared\nto basic approaches. This review summarizes current trends and\nprovides clear research directions to support the development of\nmore reliable and clinically meaningful burn-depth assessment\nsystems.\nIndex Terms—burn depth assessment; machine learning; deep\nlearning; imaging modalities; wound analysis; medical image\nprocessing.\nI. INTRODUCTION\nBurn-depth assessment is one of the most important steps\nin early burn care, guiding decisions about debridement,\ngrafting, and follow-up. However, depth is difficult to judge\naccurately because burn wounds change over the first 24–\n72 hours as tissue in the “zone of stasis” deteriorates. Only\nvisual assessment is often inaccurate, particularly when color\ndoes"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 1,
"chunk_index": 1,
"token_count": 512,
"text": " guiding decisions about debridement,\ngrafting, and follow-up. However, depth is difficult to judge\naccurately because burn wounds change over the first 24–\n72 hours as tissue in the “zone of stasis” deteriorates. Only\nvisual assessment is often inaccurate, particularly when color\ndoes not indicate deeper tissue viability or perfusion. Early\ndiagnosis of burn depth is important because delayed or faulty\nassessment lengthens healing period, risks infection, and may\nlead to unnecessary surgery. With improved imaging hardware,\ndata-driven machine learning, and deep learning techniques,\nresearchers aim to provide faster and reliable feedback. The\nfield is still fragmented as the studies differ in imaging\nconditions, dataset size, labeling methods, preprocessing steps,\nand evaluation metrics. As a result, performance comparisons\nacross papers remain unreliable, and are still not widely used\nin clinical settings. [1], [2].\nThe motivation for this review comes from the absence of\nsingle, clear analysis that links imaging techniques, datasets,\nmodel selection, preprocessing stages, and real-world con-\nstraints. Existing reviews often focus on certain modalities or\nlimited technical concerns, while recent model based studies\nintroduce new models without placing them in a wider clinical\nor methodological context. This creates a clear research gap\nthat is the community needs a unified, clinically grounded\nreview covering both imaging and AI viewpoints. To address\nthis gap, we collect recent burn-depth-related studies including\nreview articles, synthesize the findings from them and present\nthem in a structured, comparable, and clinically meaningful\nway. Fig. 1 shows the taxonomy used in this review. Burn-\ndepth assessment is organized by five non-invasive imaging\nmodalities and seven pre-processing steps and applied to five\ncategories of learning models. In this study, We aim to provide\na clear and useful reference for both researchers and clinicians,\nhelping the community build more reliable, interpretable, and\nclinically meaningful burn-depth assessment systems.\nThe main contributions of this review are:\n•A concise taxonomy linking imaging modalities, prepro-\ncessing stages, and model categories to clinical decision\nneeds.\n•A compact, integrated summary of imaging modalities,\nML/DL techniques, and popular trends in recent burn-\ndepth diagnosis related studies.\n•A conceptual experiment to show the impact of prepro-\ncessing on model performance.\n•Identification of common research gaps and focused\nresearch directions to guide future work on physiology-\naware modeling, multimodal fusion,"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 1,
"chunk_index": 2,
"token_count": 169,
"text": " imaging modalities,\nML/DL techniques, and popular trends in recent burn-\ndepth diagnosis related studies.\n•A conceptual experiment to show the impact of prepro-\ncessing on model performance.\n•Identification of common research gaps and focused\nresearch directions to guide future work on physiology-\naware modeling, multimodal fusion, fairness, data-\nefficient learning, and multi-site validation.\nII. CLINICAL ANDIMAGINGBACKGROUND\nIt is crucial to comprehend how burn injuries respond\nbiologically and why evaluating them is difficult before ad-\ndressing imaging technologies or machine-learning algorithms.\nOver the initial weeks, burn wounds change rapidly, and their\nFig. 1. Taxonomy of Burn-Depth Assessment using ML/DL approaches.\n979-8-3315-6135-2/26/$31.00 © 2026 IEEE"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 2,
"chunk_index": 0,
"token_count": 512,
"text": "outward appearance frequently conceals their actual depth.\nThe main reason imaging-based assessment has grown in\nsignificance in contemporary burn care is this discrepancy\nbetween what is apparent externally and the actual level of\ninternal tissue damage.\nA. Burn Depth Physiology\nBurn depth indicates the amount that the heat injury has\ndamaged the skin and its microvasculature. The ability of\ndermal adnexal units—such as hair follicles, sweat glands,\nand small vessels—to survive determines whether a wound\ncan heal naturally or needs surgery [3].\n•Superficial Burns (First-Degree):It only affects the\nepidermis andrecovery takes place in a few of days.\n•Superficial Partial-Thickness Burns:Upper dermis is\naffected,capillary refill is intact and healing needs two to\nthree weeks.\n•Deep Partial-Thickness Burns:Damage extends deeper,\nreducing circulation; many wounds require grafting [4].\n•Full-Thickness Burns:Complete damage of the dermis\nand epidermis; no chance of healing without surgery on\nits own.\nB. Imaging Techniques for Depth Assessment\nTo obtain objective physiological nuances that the hu-\nman eye is unable to consistently detect, a number of non-\ninvasive imaging devices have been developed. Each modality\nforms the basis for evaluation based on machine learning by\nmeasuring a distinct structural, neurological, or biochemical\ncharacteristic.\n•Infrared / Thermal Imaging:Determines variations in\nskin-surface temperature. Because of reduced circulation,\nfull-thickness burns are frequently unusually cool [5].\n•Multispectral and Hyperspectral Imaging:Estimates\noxygenation, hemoglobin concentration, and tissue com-\nposition by capturing reflectance at various wavelengths;\nthese characteristics are highly connected with sustain-\nability and healing capability [1].\n•Ultrasound:Helps determine burn depth by providing\ncross-sectional views of skin thickness, edema, and struc-\ntural disruption [5], [6].\n•Optical Coherence Tomography (OCT):Enables a\nthorough evaluation of partial-thickness injuries by pro-\nviding high-resolution imaging of the epidermal–dermal\njunction [5].\n•Raman Spectroscopy:Allows for the early detection of\nburn severity by identifying biochemical indicators like\nprotein structural alterations and collagen denaturation\n[5], [6].\n•Laser Speckle Contrast Imaging (LSCI):Identifies\nnon-viable tissue with decreased blood flow and"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 2,
"chunk_index": 1,
"token_count": 512,
"text": " [5].\n•Raman Spectroscopy:Allows for the early detection of\nburn severity by identifying biochemical indicators like\nprotein structural alterations and collagen denaturation\n[5], [6].\n•Laser Speckle Contrast Imaging (LSCI):Identifies\nnon-viable tissue with decreased blood flow and maps\nmicrovascular perfusion in real time [6].\nTemperature, perfusion, structural integrity, and biochemical\ncomposition are all quantifiable markers of tissue state that\nthese imaging modalities offer, providing a solid foundation\nfor computational depth prediction.\nC. Why Burn Depth Assessment Is Challenging\nConsidering burn wounds alter biologically over time, early\nassessment of burn depth is still challenging despite decades of\nclinical understanding. Perfusion may change throughout the\nfirst 48 hours, inflammation may mask underlying damage, and\nits external appearance usually does not accurately reflect the\nlevel of microvascular damage. As a result, observers’ ability\nto visually differentiate between superficial-partial and deep-\npartial burns varies significantly, especially in the early stages\n[6], [7].\nClinical and technology literature highlights this diagnostic\nambiguity in great detail, reporting uneven precision in visual\ninspection and emphasizing the need for objective techniques\n[1], [5]. The utilization of imaging devices and machine-\nlearning algorithms that can analyze temperature variations,\nspectral characteristics, microvascular circulation, and tissue\nshape with significantly higher precision than the human eye\nis motivated by these constraints.\nIII. METHODOLOGYOVERVIEW\nIn this review, we collected papers from electronic database\n(IEEE Xplore, ScienceDirect, SpringerLink, MDPI, Nature,\nand Wiley) within 2020–2025. Keywords included burn depth,\nburn severity, wound depth, and wound severity. Studies were\nincluded if they are open access and focused on burn-depth\nor severity assessment and used ML or DL with validation on\nhuman or animal data. Exclusion criteria removed non-skin\nburn, non-depth outcomes, and papers lacking methodological\ndetails. The resulting 20 papers formed the basis of the anal-\nysis and summarized. From the selected papers, we identified\nstate-of-the-art models and current trends in pre-processing\ntechniques. We designed a controlled experiment to quantify\npre-processing effects across multiple model architectures as\nshown in Fig.2. The experiment compares two pre-processing\napproaches: basic (minimal processing) and advanced ( burn-\nfocused steps). Four SOTA models were evaluated: two deep\nlearning"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 2,
"chunk_index": 2,
"token_count": 266,
"text": " in pre-processing\ntechniques. We designed a controlled experiment to quantify\npre-processing effects across multiple model architectures as\nshown in Fig.2. The experiment compares two pre-processing\napproaches: basic (minimal processing) and advanced ( burn-\nfocused steps). Four SOTA models were evaluated: two deep\nlearning (ResNet-50 and Vision Transformer) and two tradi-\ntional machine learning (SVM and Logistic Regression). This\nstructured approach provides clear evidence of pre-processing\nimportance in burn depth assessment.\nIV. LITERATUREREVIEW\nA. Summary of Collected Review Studies\nDespite increasing interest in burn depth assessment, exist-\ning reviews suffer from several key limitations. Most reviews\nfocus narrowly on individual modalities, without integrating\ninsights across imaging techniques. Dataset details are often\nsuperficial, lacking discussion on pre-processing strategies\n[5], [6]. Method-level reviews typically emphasize algorithm\nnames (e.g., SVM, CNN) rather than architectural principles,\nclinical adaptability, or evaluation pipelines [8]. Only a few\nreviews attempt taxonomic mapping [1], and none offer a com-\nprehensive framework connecting imaging modalities, pre-\nprocessing, and model types. Additional limitations include\nmixing human and animal studies without distinction [8], and\n2"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 3,
"chunk_index": 0,
"token_count": 512,
"text": "Fig. 2. Workflow diagram for Burn-Depth Assessment using ML/DL.\nold coverages (pre-2021) [7] limit their relevance to current\ntrends. This review addresses these gaps by:\n•reviewing recent papers (2020–2025), including both re-\nviews and model based studies;\n•introducing a structured taxonomy linking imaging\nmodalities, pre-processing techniques, and ML/DL archi-\ntectures;\n•summarizing dataset features, types of image, and pre-\nprocessing pipelines in dedicated sections;\n•comparing performance across models using standardized\nmetrics;\n•pointing implementation trends, real-world challenges,\nand future research opportunities.\nB. Technique-wise Summary of Implementation-Based Papers\n1) Deep Learning on RGB Burn Photographs:Most im-\nplementations rely on standard clinical photographs and CNN-\nbased classification or segmentation. Transfer learning is fre-\nquently employed to stabilize training on small datasets [9]–\n[11]. Attention-enhanced networks [12] improve subtle-depth\ndiscrimination, especially for superficial-partial vs deep-partial\nburns. Segmentation-focused studies such as [13] and semi-\nsupervised methods like [14] reduce annotation dependency\nand produce pixel-level maps, which are clinically useful but\nstill sensitive to lighting, camera variability and centre based\ndata.\n2) Human-Centric and Explainable AI:The work in [15]\nfocuses interpretability over accuracy.The model’s explainable\nfeatures enhance trust and facilitate the identification of errors,\nwhich is critical for real-world application.\n3) Non-RGB Physiological or Structural Imaging:\nUltrasound-based deep learning models [16] capture subsur-\nface architecture, while thermal-imaging models [17] repre-\nsent perfusion and temperature variation. Here challenges are\nstandardization, calibration, and small datasets.\n4) Spectroscopy-Based ML (Raman / Resonance Raman):\nThe approach in [18] categorize the severity of burns using\nspectral signatures of metabolic alterations. These approaches\nare light independent and identify depth before surface imag-\ning. However, they require specialized probes and have been\nvalidated mostly on ex vivo or animal models.\n5) Hyperspectral / Multispectral Perfusion Imaging:Hy-\nperspectral and multispectral studies [19]–[22] measure re-\nflectance across wavelengths to estimate hemoglobin levels,\noxygenation, and tissue viability. Earlier works use feature-\nengineered physiological maps with classical"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 3,
"chunk_index": 1,
"token_count": 512,
"text": " animal models.\n5) Hyperspectral / Multispectral Perfusion Imaging:Hy-\nperspectral and multispectral studies [19]–[22] measure re-\nflectance across wavelengths to estimate hemoglobin levels,\noxygenation, and tissue viability. Earlier works use feature-\nengineered physiological maps with classical ML, while\nnewer studies incorporate CNNs and short-wave IR for intra-\noperative guidance.\n6) Data-Efficient and Semi-Supervised Learning:Papers\nsuch as [11] and [14] directly address the scarcity of labelled\nburn images. They use transfer learning, strong augmentation,\ncontrastive learning and uncertainty-aware pseudo-labelling\nwhich reduces annotation burden though performance is still\nnot confirmed.\nC. Dataset and Image Pre-Processing Techniques\n1) Dataset Sources and Image Types:The implementation\nstudies in this review collectively cover a wide range of\nimaging setups and data sources. Most models are trained on\nclinical RGB photographs of human burn wounds acquired\nin hospital settings or outpatient clinics(e.g., [9]–[14]), while\na smaller group relies on advanced spectral imaging such as\nhyperspectral imaging (HSI) [19], [20], multispectral or short-\nwave infrared (SWIR) imaging [21], [22], and thermal infrared\nimaging [17]. Ultrasound-based approaches form another im-\nportant subgroup, both for real-time depth classification and\nexplainable AI frameworks [15], [16], and several studies use\nRaman spectroscopy on porcine skin as an experimental model\nof multi-depth burns [18].\nIn terms of subject type, the majority of datasets are\nhuman clinical cohorts (RGB, HSI, MSI, thermal) collected\nprospectively or retrospectively in burn centers [9], [10], [13],\n[17], [19]–[22]. A smaller subset uses ex vivo or in vivo\nanimal models: both Raman studies operate on porcine skin\nwith controlled burn depths [18], and at least one hyperspectral\nand one multispectral system are first validated in preclinical\nor mixed porcine–clinical settings [19], [22].\nOnly a few works explicitly name reusable datasets. The\nBurns BIP US databaseis used in the spatial attention-based\nresidual network for human burn identification [12], and\nseveral RGB-based studies build on a shared burn image col-\nlection of 2,080 labelled RGB images with four depth classes\n"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 3,
"chunk_index": 2,
"token_count": 211,
"text": "Only a few works explicitly name reusable datasets. The\nBurns BIP US databaseis used in the spatial attention-based\nresidual network for human burn identification [12], and\nseveral RGB-based studies build on a shared burn image col-\nlection of 2,080 labelled RGB images with four depth classes\n[9]–[11]. However, fully open, benchmark-style datasets re-\nmain rare; most cohorts are single-centre, locally maintained\nimage collections that are not publicly released, which limits\nreproducibility and direct comparison across models. A high-\nlevel summary of dataset sources and imaging types is given\nin Table I.\n2) Pre-Processing Techniques:Pre-processing steps across\nthe implementation studies are broadly similar despite differ-\nences in imaging modality. Most papers apply a small but\nessential set of operations to stabilize image quality, reduce\nnoise, and ensure consistent input dimensions before train-\ning the model. These steps are especially important because\nclinical burn datasets are small, heterogeneous, and often\n3"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 4,
"chunk_index": 0,
"token_count": 512,
"text": "TABLE I\nCHARACTERISTICS OFDATASETS FORBURNDEPTHASSESSMENT\nDataset Category / Source Ref. Subject Type Modality Labeling Strategy\n(Ground Truth)\nSample Size Potential Biases\nEthnic Skin-Tone Clinical Set [9], [10] Human RGB clinicians (healthy vs depth\nclasses)\n1900–2080 Images Ethnic representation bias\nPediatric Care Burn Database [11] Human RGB Expert Visual Assessment\n(depth classes)\n13,715 Images Age-specific (pediatric)\nskew\nBurns BIP US database [12] Human RGB Specialist(Depth classes\nand graft based)\n94 (Aug. 6000) Data augmentation bias\nHangzhou Hospital dataset [13] Human RGB Clilinicians( Pixel level An-\nnotation)\n516 Images single hospital, subjectivity\nSmartphone-based Field Data [14] Human RGB Experts( Pixel level, labeled\nand pseudo labeled)\n1142 Images smartphone model varia-\ntion, Sensor/Lighting varia-\ntion\nPorcine B-mode ultrasound data [15], [16] Porcine Ultrasound By histology (Porcine burn\ndepth)\n- Animal-to-human gap, lab\nsetup bias\nIR-Thermal Implementation Set [17] Human Thermal Clinical depth labels 41 cases Environment temperature\nnoise\nPorcine Ex-vivo Dataset [18] Porcine Raman Histology (5 classes) 126 samples Non-living tissue bias\nMicro-vascular Perfusion Set [19] Human HSI Clinical burn degree groups - processing pipeline depen-\ndence\nAcute Burn Recovery dataset [20] Human HSI Outcome-based: heal<\n21d vs graft>21d\n49 Patients Small demographic range\nMulti-Spectral SWIR dataset [21] Human SWIR Biopsy (depth classes) 11 pts, 273 ROIs, 21 biop-\nsies\nDevice-specific spectra\nIntra-operative Guidance Set [22] Porcine Multispectral Surgical observation (vi-\nable vs non-viable wound)\n- Operating Room lighting\nbias\ncaptured under varying lighting or sensor conditions. The most\ncommonly reported techniques in the implementation papers\ninclude:\n•Intensity normalization and resizing:Nearly all studies\nscale images to a fixed resolution and normalize pixel\nor spectral values to reduce illumination variability and\nstabilize CNN training (e.g., [9], [10], [13]).\n•Data augmentation:To compensate for limited datasets,\nmany RGB and DL-based works apply flips"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 4,
"chunk_index": 1,
"token_count": 506,
"text": "Intensity normalization and resizing:Nearly all studies\nscale images to a fixed resolution and normalize pixel\nor spectral values to reduce illumination variability and\nstabilize CNN training (e.g., [9], [10], [13]).\n•Data augmentation:To compensate for limited datasets,\nmany RGB and DL-based works apply flips, rotations,\nbrightness/contrast jitter, and random cropping. This is\none of the most frequently used techniques across the\nRGB-based models [9]–[12].\n•Segmentation or ROI extraction:Several pipelines\nisolate the burn region using U-Net-style segmentation,\nthresholding, or manual cropping so the model focuses\non the wound rather than the background [13], [19].\n•Filtering / denoising:Applied mainly in spectral or\nultrasound works, smoothing or baseline correction helps\nreduce sensor noise and improve spectral consistency\n[18]–[20].\n•Color-space or derived map transformations:A\nsmaller group of studies transforms RGB images (e.g.,\nto HSV/Lab) or derives perfusion/oxygenation maps in\nHSI and MSI data to emphasize clinically meaningful\ntissue patterns [10], [19].\nOverall, normalization/resizing and data augmentation are\nclearly the most widely adopted steps, followed by ROI\nextraction, then filtering and colour-space transforms. These\nrelative frequencies are illustrated in the accompanying pre-\nprocessing bar chart (Fig. 3), providing a visual summary\nof how often each operation appears across the reviewed\nimplementation studies.\nFig. 3. Pre-processing technique frequency across the reviewed studies.\nV. EXPERIMENT ANDRESULT ANALYSIS\nThe performance trends of earlier burn-depth evaluation\nstudies are compiled in this paragraph. The assessment mea-\nsures that are often employed across modalities are highlighted\nin Table II, along with stated accuracy levels and methodolog-\nical decisions. All things considered, the table offers a precise\nstandard to place our suggested methodology within the body\nof current literature. From this summary, we selected four\nSOTA models ResNet50, ViT, SVM, and Logistic Regression\nfor running a simple experiment with two pre-processing\nsetups: Basic and Advanced. Basic pre-processing used only\nstandard steps: resize (224×224) and normalization. Advanced\nadded burn-focused steps such as mild denoising, HSV/LAB\ncolor conversion, CLAHE contrast enhancement and ROI crop\nto make burn regions clearer and reduce background/lighting\n4"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 5,
"chunk_index": 0,
"token_count": 512,
"text": "TABLE II\nCOMPARATIVEANALYSIS OFEXISTINGLITERATURE\nRef. Model Preprocessing Key Strengths Limitations Eval. Metrics Accuracy Clinical Val.\n[9] ResNet50 Resizing, TL\nfine-tune\nHigh accuracy in\nRGB\nNo explainability Accuracy 99.3%, 97.1% No\n[10] ResNet50,VGG16 Elastic Transform,\nRotation, Noise In-\njection\nRobust features High compute cost Acc, F1 95.43%, 85.67% No\n[11] SSL,FL,TL,GAN Hist. Equalization,\nAugmentation\nEfficient for small\ndata\nOverfitting risk Accuracy 88.5% No\n[12] ResNeXt(spatial attention) Resize, normaliza-\ntion\nHigh sens. for de-\ngree & graft\nNo video feed test Sensitivity 97.22%,99.14% Yes\n[13] ResNet50,ResNet101 Resize, Crop, Aug-\nmentation\nAuto-boundary de-\ntect.\nModerate IOU for\nmulti-depth\nIoU 0.5144 No\n[14] SBCU-Net Resize, rotate, crop Efficient unlabeled\nlearning\nLabeled data depen-\ndency\nmIoU 72.88% Yes\n[15] XAI Normalization,\nCrop, Resize\nExplains ”Why”\n(XAI)\nPorcine only, Small\ndataset\nAccuracy 94.0% No\n[16] Encoder-Decoder CNN Downsampling,\nflipping\nDeep tissue pene-\ntration\nSmall data Acc, Sen, Spec 99%,98%,100% No(Porcine skin)\n[18] SVM, PLS-DA Noise reduction,\nbaseline correction\nBiochemical accu-\nracy\nEx-vivo model only Accuracy 99% No (Ex-vivo)\n[19] Physiological Model 3D surface scan,\nnormalization\nNon-invasive deep\ntissue visualization\nHigh equipment\ncost\nPerfusion index 3D mapping success Yes\n[20] Statistical Correlation Normalization 21-day prediction Environment light\nsensitivity\nLDI Correlation High agreement Yes\n[22] DCNN (Modified SegNet) Image registration,\npixel labeling\nReal-time surgical\nguidance\nRequires costly\nhardware\nDice Score 85.5% No (Porcine model)\neffects. Public burn image datasets are quite limited, and the\nexisting"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 5,
"chunk_index": 1,
"token_count": 512,
"text": "LDI Correlation High agreement Yes\n[22] DCNN (Modified SegNet) Image registration,\npixel labeling\nReal-time surgical\nguidance\nRequires costly\nhardware\nDice Score 85.5% No (Porcine model)\neffects. Public burn image datasets are quite limited, and the\nexisting few ones are usually small and noisy, which makes the\ntask hard and makes accuracy lower. We used the Skin Burn\nDataset from kaggle—a small web-scraped collection (approx.\n1300 images) labeled by burn degree (0/1/2) [23]. The dataset\nis limited in size and highly variable, the task is genuinely\nhard—so accuracies are not so high. The performance of\napplied deep learning and traditional machine learning models\nfor burn depth classification under various pre-processing tech-\nniques is compared in the table III. Results indicate advanced\npre-processing generally worked better for classification for all\nmodels. Only Vision Transformer dropped performance, likely\nbecause it is more sensitive to over-sharpening and unnatural\ncontrast of advanced pre-processing and may need retuning.\nOverall, making the burn area more visible before training\nhelped most models to increase accuracy.\nVI. CHALLENGES ANDRESEARCHDIRECTIONS\nProper burn depth assessment is challenging because\nwounds change over the first 24–72 hours, and surface color\noften fails to show actual tissue damage. Although recent stud-\nTABLE III\nPERFORMANCECOMPARISON OFIMAGECLASSIFICATIONMODELS\nModel Prec. Rec. F1 Acc.\nResNet50 (Basic) 0.706 0.707 0.706 0.707\nResNet50 (Advanced) 0.738 0.736 0.733 0.736\nViT (Basic) 0.743 0.739 0.741 0.739\nViT (Advanced) 0.718 0.715 0.712 0.715\nSVM (Basic) 0.720 0.722 0.719 0.722\nSVM (Advanced) 0.815 0.804 0.802 0.804\nLogistic Regression (Basic) 0.672 0.674 0.673 0.674\nLogistic Regression (Advanced) 0.756 0.755 0.755 0.755\nies use different imaging modalities with ML/DL"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 5,
"chunk_index": 2,
"token_count": 345,
"text": "0.802 0.804\nLogistic Regression (Basic) 0.672 0.674 0.673 0.674\nLogistic Regression (Advanced) 0.756 0.755 0.755 0.755\nies use different imaging modalities with ML/DL approaches,\nmost of them rely on subjective or delayed-healing labels,\nwhich makes depth prediction inconsistent across devices and\nclinical settings. The key challenges of burn-depth assessment\nare listed below:\n•Technical issues:Datasets continue to be small and\nunbalanced, and performance varies with changes in\nlighting and devices.\n•Clinical and workflow constraints:Metrics often do not\nindicate actual graft decisions because burn appearance\nis impacted by ointments, edema, and surgical circum-\nstances.\n•Dataset limitations:There is limited representation\nacross skin tones, burn types, and age ranges, and labels\nare inconsistent.\n•Lack of standardization:Poor imaging conditions, user\nagreement, and data splits reduces comparability.\nBuilding on these challenges, the main future research direc-\ntions are outlined below:\n•Develop models that incorporate perfusion, adnexal struc-\ntures, and temporal depth progression instead of relying\nonly on surface appearance.\n•Combine complementary imaging modalities and use\ndata-efficient approaches such as semi-supervised, self-\nsupervised, or uncertainty-aware learning.\n•Adopt prospective, multi-site clinical evaluations that\nhave impact on grafting decisions, healing time, and\nworkflow efficiency.\n•Use federated or privacy-preserving techniques to safely\naggregate data among burn centers and enhance fairness\nwith skin-tone-inclusive datasets.\n5"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 6,
"chunk_index": 0,
"token_count": 512,
"text": "•Design reliable point-of-care and intra-operative systems\nwith stable capture protocols, quality assurance, and fast\nfeedback for real-time use.\nVII. CONCLUSION\nConsidering burn wounds vary over time, differ between\npatients, and frequently lack accurate ground-truth labels,\nburn-depth evaluation is still difficult. The majority of the evi-\ndence comes from tiny, single-center, or retrospective datasets,\ndespite the fact that recent research employing RGB, thermal,\nHSI/MSI, ultrasound, Raman spectroscopy, and deep learning\ndemonstrated promising performance. The advantages and\ndisadvantages of various imaging techniques, datasets, model\nselections, preprocessing techniques, evaluation procedures,\nand new explainability approaches are addressed in this review.\nIn order to advance the field toward safe and reliable use,\nthese insights highlight the necessity of consistent reporting,\ndiversified data, enhanced validation, and clinically relevant\nmetrics.\nAll things taken into consideration, machine learning clearly\nhas the potential to facilitate quicker and more reliable burn-\ndepth evaluations; but, wider clinical use will necessitate\nmore solid evidence, carefully planned multi-site evaluations,\nand technologies that seamlessly integrate into actual clinical\nworkflows. With this review, we present a comprehensive\nreference that emphasizes methodological developments in\npresent-day research, compares performance with different\npre-processing techniques, and describes results. This synthe-\nsis seeks to direct the creation of more dependable, com-\nprehensible, and clinically significant burn-depth assessment\nmodels by describing real-world issues and targeted research\napproaches. As physiology-aware learning, multimodal imag-\ning, fairness, and real-time deployment develop, AI-driven\nsystems can become trustworthy support in both emergency\nand surgical procedures.\nREFERENCES\n[1] R. H. Wilson, R. Rowland, G. T. Kennedy, C. Campbell, V . C. Joe,\nT. L. Chin, D. M. Burmeister, R. J. Christy, and A. J. Durkin, “Review\nof machine learning for optical imaging of burn wound severity\nassessment,”Journal of Biomedical Optics, vol. 29, no. 2, p. 020901,\n2024. [Online]. Available: https://doi.org/10.1117/1.JBO.29.2.020901\n[2] P. Bhattachan, Z. Ricciuti, F. Khalaf, and M. G. Jeschke, “The"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 6,
"chunk_index": 1,
"token_count": 512,
"text": " 020901,\n2024. [Online]. Available: https://doi.org/10.1117/1.JBO.29.2.020901\n[2] P. Bhattachan, Z. Ricciuti, F. Khalaf, and M. G. Jeschke, “The role of\nartificial intelligence in burn assessment, complication diagnosis, and\noutcome prediction: A narrative review,”Burns & Trauma, p. tkaf071,\n2025. [Online]. Available: https://doi.org/10.1093/burnst/tkaf071\n[3] D. Church, S. Elsayed, O. Reid, B. Winston, and R. Lindsay, “Burn\nwound infections,”Clinical Microbiology Reviews, vol. 19, no. 2, pp.\n403–434, 2006.\n[4] D. M. Jackson, “The diagnosis of the depth of burning,”British Journal\nof Surgery, vol. 40, no. 164, pp. 588–596, 1953.\n[5] N. Farhan, Z. Hassan, M. A. M. Ali, Z. Alqalaf, R. E. Rasul, and\nS. Jeffery, “How can technology improve burn wound care: A review\nof wound imaging technologies and their application in burns-uk\nexperience,”Diagnostics, vol. 15, no. 17, p. 2277, 2025. [Online].\nAvailable: https://doi.org/10.3390/diagnostics15172277\n[6] H. Li, Q. Bu, X. Shi, X. Xu, and J. Li, “Non-invasive medical\nimaging technology for the diagnosis of burn depth,”International\nWound Journal, vol. 21, no. 1, p. e14681, 2024. [Online]. Available:\nhttps://doi.org/10.1111/iwj.14681\n[7] N. T. Liu and J. Salinas, “Machine learning in burn care and research: A\nsystematic review of the literature,”Burns, vol. 41, no. 8, pp. 1636–1641,\n2015. [Online]. Available: https://doi.org/10.1016/j.burns.2015.07.001\n[8] S. Huang,"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 6,
"chunk_index": 2,
"token_count": 512,
"text": "atic review of the literature,”Burns, vol. 41, no. 8, pp. 1636–1641,\n2015. [Online]. Available: https://doi.org/10.1016/j.burns.2015.07.001\n[8] S. Huang, J. Dang, C. C. Sheckter, H. A. Yenikomshian, and\nJ. Gillenwater, “A systematic review of machine learning and\nautomation in burn wound evaluation: A promising but developing\nfrontier,”Burns, vol. 47, no. 8, pp. 1691–1704, 2021. [Online].\nAvailable: https://doi.org/10.1016/j.burns.2021.07.007\n[9] A. Abubakar, H. Ugail, and A. M. Bukar, “Assessment of human skin\nburns: A deep transfer learning approach,”Journal of Medical and\nBiological Engineering, vol. 40, no. 3, pp. 321–333, 2020. [Online].\nAvailable: https://doi.org/10.1007/s40846-020-00520-z\n[10] A. Abubakar, H. Ugail, K. M. Smith, A. M. Bukar, and A. Elmahmudi,\n“Burns depth assessment using deep learning features,”Journal of\nMedical and Biological Engineering, vol. 40, no. 6, pp. 923–933, 2020.\n[Online]. Available: https://doi.org/10.1007/s40846-020-00574-z\n[11] H. Shin, H. Shin, W. Choi, J. Park, M. Park, E. Koh, and H. Woo,\n“Sample-efficient deep learning techniques for burn severity assessment\nwith limited data conditions,”Applied Sciences, vol. 12, no. 14, p.\n7317, 2022. [Online]. Available: https://doi.org/10.3390/app12147317\n[12] D. P. Yadav, T. Aljrees, D. Kumar, A. Kumar, K. U. Singh, and T. Singh,\n“Spatial attention-based residual network for human burn identification\nand classification,”Scientific Reports, vol. 13, no."
},
{
"filename": "1040_paper_2.pdf",
"page_number": 6,
"chunk_index": 3,
"token_count": 512,
"text": "12147317\n[12] D. P. Yadav, T. Aljrees, D. Kumar, A. Kumar, K. U. Singh, and T. Singh,\n“Spatial attention-based residual network for human burn identification\nand classification,”Scientific Reports, vol. 13, no. 1, p. 12516, 2023.\n[Online]. Available: https://doi.org/10.1038/s41598-023-39618-0\n[13] H. Liu, K. Yue, S. Cheng, W. Li, and Z. Fu, “A framework\nfor automatic burn image segmentation and burn depth diagnosis\nusing deep learning,”Computational and Mathematical Methods\nin Medicine, vol. 2021, p. 5514224, 2021. [Online]. Available:\nhttps://doi.org/10.1155/2021/5514224\n[14] D. Zhang and J. Xie, “Semi-supervised burn depth segmentation\nnetwork with contrast learning and uncertainty correction,”\nSensors, vol. 25, no. 4, p. 1059, 2025. [Online]. Available:\nhttps://doi.org/10.3390/s25041059\n[15] M. J. Jacobson, D. C. Arrubla, M. R. Tricas, G. Gordillo,\nY . Xue, C. Sen, and J. Wachs, “Human-centered xai for burn\ndepth characterization,”arXiv preprint, 2023. [Online]. Available:\nhttps://arxiv.org/abs/2210.13535\n[16] S. Lee, Rahul, J. Lukan, T. Boyko, K. Zelenova, B. Makled, C. Parsey,\nJ. Norfleet, and S. De, “A deep learning model for burn depth classifica-\ntion using ultrasound imaging,”Journal of the Mechanical Behavior of\nBiomedical Materials, vol. 125, p. 104930, 2022. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S1751616121005610\n[17] J. de Haan, M. Stoop, P. P. M. van Zuijlen, and A. Pijpe,\n“Thermal imaging for burn wound depth assessment: A mixed-methods\nimplementation study"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 6,
"chunk_index": 4,
"token_count": 512,
"text": ".com/science/article/pii/S1751616121005610\n[17] J. de Haan, M. Stoop, P. P. M. van Zuijlen, and A. Pijpe,\n“Thermal imaging for burn wound depth assessment: A mixed-methods\nimplementation study,”Journal of Clinical Medicine, vol. 13, no. 7, p.\n2061, 2024. [Online]. Available: https://doi.org/10.3390/jcm13072061\n[18] H. Ye, Rahul, U. Kruger, T. Wang, S. Shi, J. Norfleet, and S. De,\n“Raman spectroscopy accurately classifies burn severity in an ex vivo\nmodel,”Burns, vol. 47, no. 4, pp. 812–820, 2021. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S0305417920305052\n[19] J. Marotz, T. Schulz, S. Seider, D. Cruz, A. Aljowder,\nD. Promny, G. Daeschlein, T. Wild, and F. Siemers, “3d-\nperfusion analysis of burn wounds using hyperspectral imaging,”\nBurns, vol. 47, no. 1, pp. 157–170, 2021. [Online]. Available:\nhttps://doi.org/10.1016/j.burns.2020.06.001\n[20] T. Schulz, J. Marotz, S. Seider, S. Langer, S. Leuschner, and F. Siemers,\n“Burn depth assessment using hyperspectral imaging in a prospective\nsingle center study,”Burns, vol. 48, no. 5, pp. 1112–1119, 2022.\n[Online]. Available: https://doi.org/10.1016/j.burns.2021.09.010\n[21] J. Nunez, S. Mironov, B. Wan, A. Hazime, A. Clark, C. Akarichi,\nK. Abdelfattah, S. Korlakunta, S. Mandell, B. Arnoldo, R. Chan,\nJ. Goverman, R. Huebinger, C. Park, B. Evers"
},
{
"filename": "1040_paper_2.pdf",
"page_number": 6,
"chunk_index": 5,
"token_count": 346,
"text": " Wan, A. Hazime, A. Clark, C. Akarichi,\nK. Abdelfattah, S. Korlakunta, S. Mandell, B. Arnoldo, R. Chan,\nJ. Goverman, R. Huebinger, C. Park, B. Evers, D. Carlson, O. Berenfeld,\nand B. Levi, “Novel multi-spectral short-wave infrared imaging for\nassessment of human burn wound depth,”Wound Repair and\nRegeneration, vol. 32, no. 6, pp. 979–991, 2024. [Online]. Available:\nhttps://doi.org/10.1111/wrr.13221\n[22] S. Yu, J. Dwight, R. C. Siska, H. Burkart, P. Quan, F. Yi,\nS. Du, Y . Daoud, K. Plant, A. Criscitiello, J. Molnar, and J. E.\nThatcher, “Feasibility of intra-operative image guidance in burn\nexcision surgery with multispectral imaging and deep learning,”\nBurns, vol. 50, no. 1, pp. 115–122, 2024. [Online]. Available:\nhttps://doi.org/10.1016/j.burns.2023.07.005\n[23] S. Baid, “Skin burn dataset,” Kaggle dataset, [Online]. Avail-\nable: https://www.kaggle.com/datasets/shubhambaid/skin-burn-dataset.\nAccessed: Dec. 19, 2025.\n6"
},
{
"filename": "738_paper.pdf",
"page_number": 1,
"chunk_index": 0,
"token_count": 512,
"text": "2026 IEEE International Conference on Electrical, Computer & Telecommunication Engineering (ICECTE 2026)\n29–31 January 2026, Rajshahi-6204, Bangladesh\nPrivacy-Aware Feature Envy Detection through\nSurvey-Driven Federated Learning:\nFed-MLP and Fed-Hybrid Architectures\nAtiya Masuda Siddika 1,2, Zaid Rehman 3, Mohammad Shamsul Arefin 1\n1Department of CSE, Chittagong University of Engineering & Technology (CUET), Chattogram, Bangladesh\n2Institute of Information and Communication Technology, Bangladesh University of Engineering & Technology (BUET), Dhaka, Bangladesh\n3BRAC University, Dhaka, Bangladesh\nEmails: atiya.siddika@cuet.ac.bd 1,2, zaidrehman3103@gmail.com 3, sarefin@cuet.ac.bd1\nAbstract—Code smells(CS) are frequently found design issues\nwhich may not affect the execution of a system but degrades the\nquality of software. It makes the software less maintainable and\nunderstandable and also makes bug finding harder. To reduce\nthis design issue, refactoring must be performed thus leading to\nthe need of detection of code smell. Various approaches have\nbeen proposed to detect code smells; machine learning(ML)\nand deep learning(DL) based approaches are most popular\nnowadays. This paper reflects the theoretical study of various\naspects on code smell detection techniques. More specifically we\nfocused on feature envy(FE) code smell detection approaches.\nOur study shows that the approaches are mostly based on\ndeep learning and concerned with automatic feature selection,\nfeature extraction and data generation using deep learning\nmodels. To better understanding, we showed the obtained result\nof various papers with a visual representation which will help\nto precede further study. Focusing on preserving privacy, we\nalso conducted federated experiments using MLP and hybrid\nmodels over three real-world datasets—MLCQ, CrowdSmelling,\nand Landfill and compared against a centralized MLP→XGBoost\nbaseline. Our results show that federated hybrid models can\nachieve competitive performance while ensuring data privacy.\nIndex Terms—Feature Envy, Code Smell Detection, Federated\nLearning, MLP, Privacy, Deep Learning, XGBoost\nI. INTRODUCTION\nSoftware systems must continuously be updated to meet\nuser needs and business requirements. However, frequent\nupdates can lead to design degradation and the"
},
{
"filename": "738_paper.pdf",
"page_number": 1,
"chunk_index": 1,
"token_count": 512,
"text": " ensuring data privacy.\nIndex Terms—Feature Envy, Code Smell Detection, Federated\nLearning, MLP, Privacy, Deep Learning, XGBoost\nI. INTRODUCTION\nSoftware systems must continuously be updated to meet\nuser needs and business requirements. However, frequent\nupdates can lead to design degradation and the increase of\ntechnical debt, specially when object-oriented design prin-\nciples are poorly followed. These design flaws, commonly\ntermed as code smells, reduce code readability, maintainability,\nand make bug detection more challenging [1] [2]. One\npervasive code smell(CS) is ”feature envy” where a method\nshows greater interest in the data or behavior of another class\nthan in its own. To remove the CS for maintaining code\nquality, software refactoring should be employed. Refactoring\nmeans restructuring the internal structure of existing source\ncode, keeping external behaviors unchanged [3]. An important\nstep in software refactoring is to identify which part of the\ncode should be refactored, which means detecting the code\nsmells. While traditional CS detection methods often rely on\nheuristics, research is shifting towards using ML to automate\nthe detection of code smells, analyzing complex patterns and\nrelationships in the codebase. However, centralized ML mod-\nels require aggregating proprietary source code across orga-\nnizations, raising significant privacy and intellectual property\nconcerns.Federated Learning (FL)offers a promising alterna-\ntive that allows multiple parties to train models collaboratively\nby sharing parameter updates instead of raw code, reducing\nprivacy risk [4]. In this paper, we conduct a structured survey\nof feature envy detection techniques and propose federated\nlearning approaches for feature envy detection. Our main\ncontributions include:\n•The first federated framework for Feature Envy detection,\nFed-MLPandFed-Hybrid(MLP→XGBoost)\n•A comparison of centralized and federated models, show-\ning that federated learning can achieve nearly equivalent\naccuracy.\n•An integrated dataset combining three real-world sources\nfor more comprehensive evaluation.\nII. BACKGROUND\nAbout 22 code smells was defined in [1] focusing on the\nmaintainability of software systems, and a set of indicators.\nFeature envy is one of the most pervasive code smells [5]\nwhere one class “envies” another class.\nA. Feature Envy(FE)\nFE is a structural anomaly that refers to methods that show\na greater interest in the data"
},
{
"filename": "738_paper.pdf",
"page_number": 1,
"chunk_index": 2,
"token_count": 211,
"text": " the\nmaintainability of software systems, and a set of indicators.\nFeature envy is one of the most pervasive code smells [5]\nwhere one class “envies” another class.\nA. Feature Envy(FE)\nFE is a structural anomaly that refers to methods that show\na greater interest in the data and functionality of other classes\nthan in the data and responsibilities specific to their own\nclass [5]. Fig. 1 illustrates feature envy code smell, where\nClass B relies heavily on fields and methods from Class A,\nindicating an interest in Class A. This scenario represents\ncoupling due to feature envy, potentially resulting in a lack\nof cohesion. It may also violate encapsulation principles.\nSome refactoring techniques addressing feature envy are move\nmethod, introduce parameter object, extract method, introduce\nforeign method, encapsulate field etc.\nFig. 1: An Example of Feature Envy Code Smell.\n979-8-3315-6135-2/26/$31.00 © 2026 IEEE"
},
{
"filename": "738_paper.pdf",
"page_number": 2,
"chunk_index": 0,
"token_count": 512,
"text": "B. Code Smell Detection Tools\nStatic tools like JDeodorant, JMove, PMD, iPlasma,\nDECOR and others employ heuristics/metrics to detect smells\n(e.g., FE, Long Method, Data Class). They are fast and\ninterpretable and help static analysis,but they often lack\nadaptability to different projects and require access to full\nsource code, which may not be feasible in industrial contexts.\nLearning-based approaches (CNN/LSTM, GNN, CodeBERT,\nand ensembles) improved generalization by learning semantic\nand structural signals from code [6]–[11].\nC. Federated Learning (FL)\nFederated Learning (FL) is a decentralized ML approach\nthat enables several clients to collaboratively train a model\nwithout sharing raw data. Instead, only model parameters are\nexchanged, preserving data privacy. FL has been successfully\nused in domains such as healthcare, finance, and IoT, where\nsensitive data must remain local. Several FL algorithms are\n- FedAvg (simple model averaging), FedProx (adds proximal\nterm for stability), FedNova (normalizes client updates), Scaf-\nfold (uses control variates to reduce client drift), FedOpt (adap-\ntive optimizer-based aggregation), FedBN (keeps local batch\nnormalization), FedPer (personalized model layers per client),\nFedDyn (dynamic regularization for non-IID data), FedMeta\n(meta-learning for quick adaptation), FedAvgM (FedAvg with\nmomentum).\nD. Security and Privacy Risks in Centralized CS Detection\nSoftware companies have concerns over intellectual prop-\nerty and privacy because traditional centralized ML pipelines\nrequire compiling all code samples into a single repository. It\nincludes some potential risks,for example source code leaking,\nwhich could expose internal design details. Also, competitors\ncan poison data and transform the model to extract training\ndata from model parameters. To avoid such risks, organizations\ncan use FL to contribute to a shared model without revealing\ntheir proprietary source code and preserving privacy.\nE. Potentials of FL in CS Detection\nFL is basically concerned about privacy preservation data.\nSo it has potentials for the future of software analytics such\nas:\n•Privacy-Preserving Collaboration: Several companies or\nopen-source projects can train models collaboratively.\n•Cross-Repository Generalization: FL allows learning\nacross heterogeneous data distributions (non-IID\ndatasets).\n•Decreased Labeling Cost: Federated fine-tuning lets each"
},
{
"filename": "738_paper.pdf",
"page_number": 2,
"chunk_index": 1,
"token_count": 512,
"text": " of software analytics such\nas:\n•Privacy-Preserving Collaboration: Several companies or\nopen-source projects can train models collaboratively.\n•Cross-Repository Generalization: FL allows learning\nacross heterogeneous data distributions (non-IID\ndatasets).\n•Decreased Labeling Cost: Federated fine-tuning lets each\nparticipant use their partial or weakly-labeled data.\n•Model Evolution: Each client’s local model continuously\nadjusts and gets better over time through aggregation\n(FedAvg, FedProx, etc.).\nIn the context of Feature Envy, FL provides a secure mecha-\nnism for training detection models across different repositories\nwhile preserving intellectual confidentiality.\nIII. LITERATUREREVIEW\nA. Technique-wise Review of Collected Papers\nResearch on FE detection has evolved through multiple\nmethodological phases—ranging from metric-based heuristics\nto deep learning models and hybrid models. This section\nsynthesizes the approaches used by the papers, emphasizing\ntheir datasets, techniques, and evolution.\n1) Metric-Based Learning:Manually generated software\nmetrics (ATFD, LAA, FDP, LOC, WMC, CBO, RFC, LCOM)\nwere the main focus of early approaches. Fontana et al. [9]\nand Hadj-Kacem et al. [12] applied static-metric thresholds\nand rule -based learning. Dewangan et al. [11] and ˇSkipina et\nal. [13] optimized these models using ensemble ML classifiers\nsuch as Random Forest and SVM which achieved good\ninterpretability but struggled with unseen projects.\n2) Deep Learning Models:Liu et al. [8] and Guo et al.\n[14] used CNNs and DNNs for feature extraction from metrics\nor tokens. Jeevanantham et al. [15] extended this to misplaced\nfields and methods that improved representation learning but\nrequired large labeled data.\n3) Graph and Structure Based Models:These models\ncaptured inter-class relationships using graph representations.\nFontana et al. [9] introduced GNN-based refactoring, while\nZhang et al. [6] used snapshot ensembles for structural\nstability. These models enhanced contextual reasoning but\nrequired high computation.\n4) Transformer and Pre-Trained Models:Nguyen et al.\n[10] and Kim et al. [16] exploited CodeBERT / GraphCode-\nBERT embeddings, enabling cross-project generalization with\nlimited training. However"
},
{
"filename": "738_paper.pdf",
"page_number": 2,
"chunk_index": 2,
"token_count": 370,
"text": "sembles for structural\nstability. These models enhanced contextual reasoning but\nrequired high computation.\n4) Transformer and Pre-Trained Models:Nguyen et al.\n[10] and Kim et al. [16] exploited CodeBERT / GraphCode-\nBERT embeddings, enabling cross-project generalization with\nlimited training. However, heavy fine-tuning costs and GPU\nrequirements limited scalability.\n5) Hybrid and AutoML Frameworks:Rahman et al. [17]\nintegrated AutoML for move-method refactoring, while Ho\net al. [18] combined CNN + LSTM layers. Dewangan et\nal. [11] fused classical classifiers, enhancing F1 but depending\non centralized data.\n6) Federated and Privacy-Preserving Learning:Alawadi et\nal. [4] introduced FedCSD, the first federated approach for\nCSD, enabling distributed training without data sharing. Build-\ning on this, our study applies Federated MLP and Federated\nHybrid (MLP→XGBoost) for FE detection across MLCQ,\nCrowdSmelling, and Landfill datasets—addressing privacy and\nheterogeneity issues absent in prior work.\nThe comparative analysis of all reviewed studies, including\ntheir core techniques, datasets, contributions, and research\ngaps is shown in Table I.\nB. Summary of Reviewed Studies and Datasets\nTo provide a concise synthesis, Table I presents a compara-\ntive summary of key studies in Feature Envy and Code Smell\nDetection. The frequently used datasets are listed in Table II.\nIV. METHODOLOGYOVERVIEW\nA. Research Design and Study Selection\n1) Paper Collection:We conducted a literature search in\nIEEE Xplore, ACM DL, SpringerLink, Elsevier, and Zenodo\n2"
},
{
"filename": "738_paper.pdf",
"page_number": 3,
"chunk_index": 0,
"token_count": 512,
"text": "TABLE I: Aggregated Comparative Review of Feature Envy Detection Studies\nAuthors Core Technique Features Dataset Key Contributions\n/ Findings\nLimitations Future Directions\nArcelli Fontana et al.,\n2024 [9]\nGNN + Refactoring Metrics + Graph Landfill Context-aware FE\nlocalization and\nrefactoring\nGraph complexity;\nruntime\nLightweight GNNs;\nscalable graph ops\nRahman et al., 2024\n[17]\nAutoML\n(Move-Method)\nStatic Metrics Custom OSS Automated tuning\nfor FE refactoring\nLimited corpora Multi-project\nAutoML; broader\nbenchmarks\nNguyen et al., 2023 [10] Pre-trained\n(CodeBERT)\nToken\nEmbeddings\nCustom Better\ntransferability with\ncontextual code reps\nHigh compute for\nfine-tuning\nMulti-language\ndomain adaptation\nKim et al., 2023 [16] DL with real\nexamples\nTokens + AST Custom Real-world\nexamples boost\nF1/generalization\nClass imbalance Augmentation;\nlarger real datasets\nZhang et al., 2022 [6] Snapshot Ensemble\n(DL)\nMetrics + Graph Landfill Stabilized\nperformance under\nstructural shifts\nModel complexity Ensemble\nsize/hyperparam\noptimization\nHadj-Kacem et al., 2018\n[12]\nHybrid Deep\nLearning\nMetric Set CrowdSmelling Combined metrics\nand learned reps for\nCSD\nLimited\ngeneralization\nCross-project\nvalidation\nLiu et al., 2018 [7] DNN Metrics + AST Custom Deep metric\nrepresentations for\nFE\nLacks rich\nsemantics\nAdd code\nembeddings/context\nGuo et al., 2019 [14] CNN Metrics + Tokens CrowdSmelling Automatic\nrepresentation\nlearning from\ntokens\nSmall dataset Scale up; richer\ntokenization\nHadj-Kacem et al., 2019\n[19]\nV AE Representation Metrics Custom Latent features for\nsmell\ncharacterization\nInterpretability XAI/visual analysis\nLiu et al., 2019 [8] DL on Metrics Metric Set MLCQ Strong baseline\naccuracy/F1\nOverfitting Regularization;\ndropout tuning\nJeevanantham et al.,\n2022 [15]\nCNN + FCN Metrics Custom Extends to\nmisplaced\nfields/methods\nSmall data; narrow\neval\nLarger benchmarks;\nmore smells\nKhleel et al., 2022"
},
{
"filename": "738_paper.pdf",
"page_number": 3,
"chunk_index": 1,
"token_count": 512,
"text": "1\nOverfitting Regularization;\ndropout tuning\nJeevanantham et al.,\n2022 [15]\nCNN + FCN Metrics Custom Extends to\nmisplaced\nfields/methods\nSmall data; narrow\neval\nLarger benchmarks;\nmore smells\nKhleel et al., 2022 [20] DCNN +\nOversampling\nMetrics MLCQ Better F1 via class\nbalancing\nSimple model Ensembles;\nhybridization\nDewangan et al., 2022\n[11]\nEnsemble ML Metrics Landfill Higher accuracy\nwith ensemble\nlearners\nCentralized-only Federated extension\nHo et al., 2023 [18] CNN + LSTM\nFusion\nSequential\nMetrics\nCustom Captures\ntemporal/sequence\npatterns\nTraining overhead Lightweight fusion\nˇSkipina et al., 2023 [13] ML Ensemble Metrics OSS Detects FE + Data\nClass jointly\nLow transferability Cross-domain\nlearning\nAlawadi et al., 2024 [4] Federated Learning\n(FedCSD)\nMetrics Multi-project Privacy-preserving\nCSD with FedAvg\nPrototype scope Apply FL to FE;\nsecure agg/DP\nwith keywords “Feature Envy”, “Code Smell Detection”, “Ma-\nchine Learning”, “Deep Learning”, “Federated Learning” in\nthe time frame: 2018–2024. We initially retrieved 48 papers,\nfiltered down to 16. Both journal and conference papers were\nconsidered if they presented experimental or model-based\nresults.\n2) Inclusion and Exclusion Criteria:We included or ex-\ncluded the Studies for review based on the following consid-\nerations:\n•Scope:Included if the study focused onFeature Envy\norCode Smell Detectionusing Machine Learning (ML),\nDeep Learning (DL), or Federated Learning (FL); ex-\ncluded if unrelated to smell detection or purely conceptual\nin nature.\n•Method:Included if the paper showed experimental\nresults using datasets, models, or evaluation metrics; ex-\ncluded if the work was editorial, theoretical, or described\ntools without experimental evaluation.\n•Publication:Selected if published in English across\n2018–2024, peer-reviewed, or available as full-access\npreprints; excluded if non-English, abstract-only, or lacks\naccessibility.\n3) Methodology Flow:The research followed a structured\nworkflow that is shown in Fig. 2.\nB. Proposed Approach\n1) Model"
},
{
"filename": "738_paper.pdf",
"page_number": 3,
"chunk_index": 2,
"token_count": 98,
"text": " across\n2018–2024, peer-reviewed, or available as full-access\npreprints; excluded if non-English, abstract-only, or lacks\naccessibility.\n3) Methodology Flow:The research followed a structured\nworkflow that is shown in Fig. 2.\nB. Proposed Approach\n1) Model Overview:Based on trends observed in prior\nwork and considering privacy preservation, we designed and\nimplemented three experimental setups— FL-MLP, FL-Hybrid\n3"
},
{
"filename": "738_paper.pdf",
"page_number": 4,
"chunk_index": 0,
"token_count": 512,
"text": "TABLE II: Dataset Overview Across Reviewed Studies\nDataset Name Features / Format Source Language(s) Papers Using It Notes\nMLCQ Labeled methods with smell metrics\n(CSV)\nJava [8], [20] Manually annotated; strong FE\nbenchmark\nCrowdSmelling Community-labeled metric vectors\n(CSV)\nJava [12], [14], [19] Real GitHub projects; smaller\nbut diverse\nLandfill Metric + structural/graph info\n(CSV/derived)\nJava / Mixed [9], [6], [11] Balanced samples; used in\nGNN/ensemble\nJava OSS Structural graph representations\n(author-mined)\nJava / Kotlin [17], [13] GNN and AutoML refactoring\nstudies\nCustom Sets Author-curated or GitHub-mined\nrepos (harmonized)\nJava, Python, C# [10], [16], [7], [15],\n[18], [4]\nUsed in pre-trained DL and FL\nexperiments\nFig. 2: A Schematic Representation of the Research Process.\nand MLP→XGBoost, to empirically validate the effective-\nness of federated learning for Feature Envy detection. Three\ndatasets were used as three clients for FL. An Architectural\noverview of the proposed models is shown in Fig. 3.\nFL-MLP (FedAvg):Three clients independently train identi-\ncal MLP models; the federated server aggregates their weights\nusing the FedAvg algorithm to build a global model.\nFL-Hybrid (MLP→XGBoost):Clients train a federated MLP\nencoder, whose learned embeddings are sent to a central server.\nA central XGBoost classifier is then trained on these combined\nrepresentations.\nCentral MLP→XGBoost:A traditional centralized approach\ntrained only on the Landfill dataset, with the MLP acting as\na feature extractor and XGBoost as the final classifier.\nFig. 3: Architectural Overview of the Implemented Models.\n2) Algorithm Pseudocode:We also present pseudocode in\nAlgorithm 1 and Algorithm 2, outlining the communication\nand aggregation process in our federated MLP and hybrid\nmodels. These pseudocodes define the round-based training\nat the client level and server-side aggregation procedures that\nare repeated until convergence. Algorithm 1 in Fig. 4 is for\nfor FL-MLP model and Algorithm 2 in Fig. 5 combines both\ncentralized and federated settings.\nV. IMPLEMENATION\nA."
},
{
"filename": "738_paper.pdf",
"page_number": 4,
"chunk_index": 1,
"token_count": 468,
"text": "-based training\nat the client level and server-side aggregation procedures that\nare repeated until convergence. Algorithm 1 in Fig. 4 is for\nfor FL-MLP model and Algorithm 2 in Fig. 5 combines both\ncentralized and federated settings.\nV. IMPLEMENATION\nA. Datasets\n1) Description:Three public Java code smell datasets were\nused, all providing a compatible set of static software metrics\n(e.g., ATFD, LAA, FDP, LOC, WMC, CBO, LCOM, RFC,\nNOM). Each provides static software metrics at the method\nor class level, suitable for metric-based machine learning.\n1) The MLCQ (Machine Learning Code Quality) dataset\n[21] contains expert-labeled Java methods and classes\nfocused on the Feature Envy (FE) smell. It uses static\ncode metrics such as ATFD, LAA, FDP, LOC, and\nWMC. It is metric-guided and binary labeled, with about\n2,242 processed instances.\n2) The CrowdSmelling dataset [22] includes community-\nlabeled Java examples, merging the 2018 and 2019 FE\nsubsets. It uses the same metric features as MLCQ for\nconsistency and applies crowd consensus for binary la-\nbeling. After cleaning, it contains around 207 instances.\n3) The Landfill dataset [23] is derived from open-source\nJava projects tagged with smells, focusing on the FE sub-\nset. It uses similar metric-based features (ATFD, LAA,\nFDP, etc.) and binary FE labels. The processed version\nincludes about 243 labeled instances, harmonized with\nthe other datasets.\n2) Dataset Preprocessing:Each dataset shares the same\nfeature schema, enabling unified MLP training across clients\nand feature extraction for hybrid modeling. Each dataset was\npreprocessed into csv format, labeled (1 = Feature Envy, 0\n= other). Missing values imputed , numeric features z-score\nstandardized and categorical columns (if any) were one-hot\nencoded. A stratified 80/20 split (random_state=42) was\nused within each dataset, forming non-IID clients for federated\nexperiments.\n4"
},
{
"filename": "738_paper.pdf",
"page_number": 5,
"chunk_index": 0,
"token_count": 512,
"text": "B. Model Architecture and Training\nWe implemented three models for Feature Envy detection\nusing both centralized and federated setups. The Centralized\nHybrid (MLP→XGBoost) model was trained solely on the\nLandfill dataset, where a small MLP (2–3 dense layers) was\nused as a feature extractor and its penultimate layer outputs\nwere passed to an XGBoost classifier. In the FL-MLP (Fe-\ndAvg) model, we simulated three clients using the MLCQ,\nCrowdSmelling, and Landfill datasets. Each client trained an\nidentical MLP locally with shared architecture and features\n(local epochs = 3, batch size = 32, learning rate = 1e-3),\nand their weights were aggregated at the server using FedAvg\nover 20 rounds. For the FL-Hybrid model, we first trained a\nfederated MLP encoder (same FL setup), and then collected\nthe encoded feature embeddings from each client to train\na centralized XGBoost classifier. This hybrid approach uses\nXGBoost’s classification capabilities centr,ally allowing repre-\nsentation learning in a privacy-preserving distributed manner.\nFor consistency, all models applied the same metric features\nand data splits. The implementation was done in Python with\nFedML and XGBoost libraries. Evaluation metrics include\nPrecision, Recall, F1-score, and Average Precision (AP) based\non PR curves.\nVI. RESULTS ANDDISCUSSION\nA. Performance Comparison\nWe evaluated three models:FL-MLP (FedAvg),Feder-\nated Hybrid (MLP→XGBoost), andCentralized Hybrid\n(MLP→XGBoost). Evaluation was based on confusion ma-\ntrices and PR curves and it reflects each model’s classification\nability and trade-off between precision and recall.\nFig. 6 demonstrates that theFedAvg MLPperforms poorly\nin detecting the minority class (Feature Envy), with no true\nFig. 4: Algorithm 1:Fed-MLP— Federated MLP for Feature\nEnvy Detection\nRequire:Kclients; roundsT; initial global weightsw 0\nEnsure:Global modelw T\n1:Serverinitializesw←w 0\n2:fort= 1. . . Tdo\n3:Sample client subsetS t ⊆ {1, . . . , K}\n4:for eachk∈S t in parallel do\n5:Clientk:"
},
{
"filename": "738_paper.pdf",
"page_number": 5,
"chunk_index": 1,
"token_count": 512,
"text": "Global modelw T\n1:Serverinitializesw←w 0\n2:fort= 1. . . Tdo\n3:Sample client subsetS t ⊆ {1, . . . , K}\n4:for eachk∈S t in parallel do\n5:Clientk: downloadw, train MLP on localD k\n6:Update local weightsw (k) ←w−η∇L k(w)\n7:Uploadw (k) to server\n8:end for\n9:Server: FedAvg aggregation\nw←\nX\nk∈St\nnkP\nj∈St nj\nw(k)\n10:end for\n11:returnw T ←w\nFig. 5: Algorithm 2:Fed-Hybrid— Federated MLP Feature\nExtractor +Centralized XGBoost\nRequire:Kclients; roundsT; encoders{E k}; datasets{D k}\nEnsure:Global encoderE glob; centralized XGBoost classifier\n1:Initialize client encodersE k ←shared weights\n2:fort= 1. . . Tdo\n3:for eachk∈ {1, . . . , K}in parallel do\n4:TrainE k onD k to minimize local loss; obtain\nweightsE (t)\nk\n5:UploadE (t)\nk to server\n6:end for\n7:Server: FedAvg on encoders to getE (t+1)\nglob\n8:end for\n9:Feature export (local): computeZ k ←E glob(Dk)on\neach client\n10:Central (secure channel): concatenate{Z k}or summary\nstats\n11:Train XGBoost on aggregated embeddings to obtain final\nclassifier\n12:return{E glob,XGBoost}\npositives, indicating trouble in handling the strong class imbal-\nance despite high overall accuracy. TheCentralized Hybrid\nandFederated Hybridconfigurations illustrate substantial\nimprovements, achieving notable true positive counts (20 and\n18, respectively). According to PR curves, theCentralized\nHybridattains the best average precision (AP = 0.365),\nfollowed closely by theFederated Hybrid(AP = 0.358),\nindicating that FL achieves comparable performance while\npreserving data privacy.\nB. Quantitative Summary\nThe precision, recall, F1, and average precision (AP) scores\nacross all three implementations are summarized in Table III.\nThe"
},
{
"filename": "738_paper.pdf",
"page_number": 5,
"chunk_index": 2,
"token_count": 240,
"text": "followed closely by theFederated Hybrid(AP = 0.358),\nindicating that FL achieves comparable performance while\npreserving data privacy.\nB. Quantitative Summary\nThe precision, recall, F1, and average precision (AP) scores\nacross all three implementations are summarized in Table III.\nThe hybrid models significantly outperform the FedAvg MLP\nmodel, with the Federated Hybrid achieving results nearly\nidentical to the centralized setup. This demonstrates that FL\ncan yield robust feature learning while protecting privacy and\nmitigating data-sharing risks.\nTABLE III: Model Performance Summary (Precision, Recall,\nF1, AP)\nModel Precision Recall F1 AP\nCentralized Hybrid 0.963 0.967 0.965 0.365\nFederated Hybrid 0.962 0.967 0.964 0.358\nFedAvg MLP 0.939 0.969 0.953 0.023\nVII. CONCLUSION\nThis study set out to review how Feature Envy (FE) has\nbeen explored in prior research and to evaluate whether\nprivacy-preserving learning can support FE detection across\n5"
},
{
"filename": "738_paper.pdf",
"page_number": 6,
"chunk_index": 0,
"token_count": 512,
"text": "Fig. 6: Confusion Matrices and PR Curves for All Three Implementations: (a) FedAvg MLP, (b) Centralized Hybrid, (c)\nFederated Hybrid, and (d) PR Comparison.\norganizations. Through a structured literature survey, we or-\nganized existing FE detection methods by technique, examined\ncommonly used datasets, and summarized key contributions,\nlimitations, and research gaps in a unified view. Based on these\ninsights, we introduced the first federated learning framework\nfor FE detection, enabling collaborative model training without\nexposing private code. Our results show that the federated\napproach reaches accuracy comparable to centralized training,\nand the integration of three real-world datasets demonstrates\nbroader applicability. These findings suggest that federated\nlearning can serve as a practical and privacy-aware solution\nfor improving software quality.\nThe work does carry certain constraints, including commu-\nnication overhead and reliance on consistent data formatting\nacross participants. Nonetheless, it opens several meaningful\nresearch directions: incorporating GNN-based code represen-\ntations, applying stronger privacy mechanisms such as Dif-\nferential Privacy and Secure Aggregation, and extending the\nframework to detect other code smells like Data Class and\nLong Method.\nIn summary, this study positions federated learning as\na promising direction for collaborative yet secure Feature\nEnvy detection and a pathway toward more privacy-preserving\nsoftware engineering practices.\nREFERENCES\n[1] M. Fowler,Refactoring. Addison-Wesley Professional, 2018.\n[2] I. Ahmed, U. A. Mannan, R. Gopinath, and C. Jensen, “An empirical\nstudy of design degradation: How software projects get worse over\ntime,” inProceedings of the 2015 ACM/IEEE International Symposium\non Empirical Software Engineering and Measurement (ESEM), Beijing,\nChina, 2015, pp. 1–10.\n[3] T. Mens and T. Tourw ´e, “A survey of software refactoring,”IEEE\nTransactions on Software Engineering, vol. 30, no. 2, pp. 126–139, 2004.\n[4] S. Alawadiet al., “Fedcsd: A federated learning based approach for\ncode-smell detection,”IEEE Access, 2024.\n[5] M. Fowler,Refactoring: Improving the Design of Existing Code.\nAddison-Wesley, "
},
{
"filename": "738_paper.pdf",
"page_number": 6,
"chunk_index": 1,
"token_count": 512,
"text": "2004.\n[4] S. Alawadiet al., “Fedcsd: A federated learning based approach for\ncode-smell detection,”IEEE Access, 2024.\n[5] M. Fowler,Refactoring: Improving the Design of Existing Code.\nAddison-Wesley, 1999.\n[6] X. Zhang and M. Wu, “Feature envy detection with deep learning and\nsnapshot ensemble,” inInternational Conference on Data Science and\nAdvanced Analytics (DSA), 2022.\n[7] H. Liu, Z. Xu, and Y . Zou, “Deep learning based feature envy detection,”\ninProceedings of the 33rd ACM/IEEE International Conference on\nAutomated Software Engineering (ASE), 2018, pp. 385–396.\n[8] H. Liu, J. Jin, Z. Xu, Y . Zou, Y . Bu, and L. Zhang, “Deep learning based\ncode smell detection,”IEEE Transactions on Software Engineering,\nvol. 47, no. 9, pp. 1811–1837, 2019.\n[9] F. Arcelli Fontana, “Efficient feature envy detection and refactoring\nbased on graph neural network,”Springer Nature, 2024.\n[10] T. Y . Nguyen, H. Le, and C. Tran, “Pre-trained model based feature\nenvy detection,” inProceedings of the 20th International Conference on\nMining Software Repositories (MSR), 2023.\n[11] S. Dewangan, R. S. Rao, A. Mishra, and M. Gupta, “Code smell de-\ntection using ensemble machine learning algorithms,”Applied Sciences,\nvol. 12, no. 20, p. 10321, 2022.\n[12] M. Hadj-Kacem and N. Bouassida, “A hybrid approach to detect code\nsmells using deep learning,” inProceedings of ENASE, 2018, pp. 137–\n146.\n[13] M. ˇSkipina, J. Slivka, N. Luburi ´c, and A. Kova ˇcevi´c, “Automatic\ndetection of feature envy and data class code smells using machine\nlearning,”Journal of Software Maintenance and Evolution, 2023.\n[14] X. Guo, C. Shi, and H. Jiang"
},
{
"filename": "738_paper.pdf",
"page_number": 6,
"chunk_index": 2,
"token_count": 512,
"text": " Luburi ´c, and A. Kova ˇcevi´c, “Automatic\ndetection of feature envy and data class code smells using machine\nlearning,”Journal of Software Maintenance and Evolution, 2023.\n[14] X. Guo, C. Shi, and H. Jiang, “Deep semantic-based feature envy\nidentification,” inProceedings of the 11th Asia-Pacific Symposium on\nInternetware, 2019, pp. 1–6.\n[15] M. Jeevanantham and J. Jones, “Extension of deep learning-based feature\nenvy detection for misplaced fields and methods,”International Journal\nof Intelligent Engineering and Systems, vol. 15, no. 1, pp. 563–574,\n2022.\n[16] Y . Kim, A. Shah, and S. Park, “Deep learning-based feature envy\ndetection boosted by real world examples,” inProceedings of the ACM\nJoint European Software Engineering Conference and Symposium on\nthe Foundations of Software Engineering (ESEC/FSE), 2023.\n[17] A. S. Rahman, S. Hasan, and M. M. Rahman, “Detecting and resolving\nfeature envy through automl and move method refactoring,”Interna-\ntional Journal of Electrical and Computer Engineering, vol. 14, no. 3,\npp. 2031–2042, 2024.\n[18] A. Ho, A. M. Bui, P. T. Nguyen, and A. Di Salle, “Fusion of deep\nconvolutional and lstm recurrent neural networks for automated detection\nof code smells,” inProceedings of the 27th International Conference on\nEvaluation and Assessment in Software Engineering (EASE), 2023, pp.\n229–234.\n[19] M. Hadj-Kacem and N. Bouassida, “Deep representation learning for\ncode smells detection using variational auto-encoder,” inInternational\nJoint Conference on Neural Networks (IJCNN), 2019, pp. 1–8.\n[20] N. A. A. Khleel and K. Neh ´ez, “Deep convolutional neural network\nmodel for bad code smells detection based on oversampling method,”\nIndonesian Journal of Electrical Engineering and Computer Science,\nvol. 26, no. 3, pp. 1725–1735, 2022.\n[21] M."
},
{
"filename": "738_paper.pdf",
"page_number": 6,
"chunk_index": 3,
"token_count": 203,
"text": " Neh ´ez, “Deep convolutional neural network\nmodel for bad code smells detection based on oversampling method,”\nIndonesian Journal of Electrical Engineering and Computer Science,\nvol. 26, no. 3, pp. 1725–1735, 2022.\n[21] M. ˇSkipina and collaborators, “Machine learning code quality (mlcq)\ndataset,” https://zenodo.org/record/6579004, 2022, accessed 2025-11-11.\n[22] F. Arcelli Fontana and collaborators, “Landfill: Code smell dataset for\njava systems,” https://zenodo.org/record/6080422, 2021, accessed 2025-\n11-11.\n[23] C. Project, “Crowd-based code smell dataset (feature envy subset),”\nhttps://zenodo.org/record/3364574, 2019, accessed 2025-11-11.\n6"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 1,
"chunk_index": 0,
"token_count": 512,
"text": "CAM-PoEm: Contour-Aware Mamba Bottleneck with Multi-Kernel Positional\nEmbedding-based Encoder for Gastrointestinal Polyp Segmentation\nAnonymous submission\nAbstract\nAccurate segmentation of gastrointestinal polyps is vital for\nearly detection of colorectal cancer, yet remains challeng-\ning due to ambiguous boundaries, heterogeneous textures,\nand high morphological variability. While convolutional neu-\nral networks (CNNs) lack effective global context model-\ning and transformers suffer from quadratic complexity, re-\ncent state-space models (SSMs) like Mamba enable efficient\nlinear-time global modeling. However, SSMs remain inher-\nently insensitive to boundary structures that are critical for\nprecise medical segmentation. We propose CAM-PoEm, a\nnovel segmentation framework that introduces a Contour-\nAware Mamba (CAM) bottleneck, where learnable contour\ncues are integrated into Mamba’s selective scan mechanism\nto enable morphology-aware global context modeling with\nboundary sensitivity. By placing CAM in the bottleneck\nwhere fine spatial details are less and boundaries and global\nfeatures are crucial, we enhance global reasoning while pre-\nserving crucial contour details with minimal computational\ncost. To complement this, we incorporate a Multi-Kernel Po-\nsitional Embedding (MKPE) encoder that captures multi-\nscale spatial features, enabling robust local representation\nalongside global modeling. To our knowledge, this is the\nfirst application of contour-sensitive SSMs in medical im-\nage analysis. CAM-PoEm achieves state-of-the-art perfor-\nmance across four benchmark datasets—Kvasir-SEG (Dice:\n0.91) (Jha et al. 2019a), PolypGen2021 (0.88) (Ali et al.\n2021), CVC-ClinicDB (0.91)(Bernal et al. 2015), and CVC-\nColonDB (0.88) (Bernal, S ´anchez, and Vilari ˜no 2012)—us-\ning only 4.96M parameters, demonstrating strong boundary\ndelineation and high computational efficiency.\nIntroduction\nGastrointestinal (GI) polyps—abnormal growths in the\ncolon, rectum, or stomach—are established precursors to\ncolorectal cancer, a leading cause of global cancer-related\ndeaths (Djinbachian et al. 2020). Timely detection and re-\nmoval via colonoscopy can reduce incidence by up to"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 1,
"chunk_index": 1,
"token_count": 512,
"text": "normal growths in the\ncolon, rectum, or stomach—are established precursors to\ncolorectal cancer, a leading cause of global cancer-related\ndeaths (Djinbachian et al. 2020). Timely detection and re-\nmoval via colonoscopy can reduce incidence by up to 30%\n(Haggar and Boushey 2009), making accurate polyp seg-\nmentation from endoscopic images essential for early di-\nagnosis and improved clinical outcomes. Yet, segmentation\nremains challenging due to high variability in polyp ap-\npearance and indistinct boundaries with surrounding tissues\n(Pooler et al. 2023).\nDeep learning has advanced this task, but existing ap-\nproaches face key limitations. CNNs like FCNs (Long, Shel-\nhamer, and Darrell 2015), U-Net (Ronneberger, Fischer, and\nBrox 2015a), and ResUNet++ (Jha et al. 2019b) extract local\nfeatures effectively but struggle to model long-range depen-\ndencies due to limited receptive fields (Li et al. 2024c; Zhao\net al. 2024), leading to degraded performance on polyps with\nirregular boundaries (Xie et al. 2024a). Transformer-based\nmodels such as TransUNet (Chen et al. 2021) and Swin-\nUNet (Cao et al. 2021) capture global context using self-\nattention (Dosovitskiy et al. 2021; Liu et al. 2024), but their\nquadratic complexity hinders scalability, and reliance on\nnatural image pretraining introduces domain gaps in medi-\ncal settings (Vaswani et al. 2017; Kirillov et al. 2023). While\nattention-based models enhance long-range reasoning, they\noften yield redundant representations and imprecise seg-\nmentation near object boundaries due to pixel-category mix-\ning and lack of explicit contour modeling (You et al. 2025;\nLi et al. 2024a).\nState-space models (SSMs) like Mamba (Gu and Dao\n2023) have recently emerged as efficient alternatives, of-\nfering linear-time global modeling. Their applications in\nsegmentation—MambaUNet (Wang et al. 2024b), UMamba\n(Ma, Li, and Wang 2024), VM-UN"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 1,
"chunk_index": 2,
"token_count": 349,
"text": "amba (Gu and Dao\n2023) have recently emerged as efficient alternatives, of-\nfering linear-time global modeling. Their applications in\nsegmentation—MambaUNet (Wang et al. 2024b), UMamba\n(Ma, Li, and Wang 2024), VM-UNet (Ruan, Li, and Xiang\n2024)—show promise. However, current SSM-based meth-\nods remain largely insensitive to spatial boundaries and of-\nten operate at a single scale, limiting accuracy in medical\nsegmentation tasks that require fine-grained contour delin-\neation (Xu et al. 2024; Ho et al. 2025a).\nTo this end, we propose CAM-PoEm, a novel segmen-\ntation framework that integrates global context modeling\nwith explicit boundary sensitivity. Central to our approach\nis the Contour-Aware Mamba (CAM) bottleneck, which in-\ntroduces learnable contour guidance into Mamba’s selective\nscan for boundary-aware global reasoning. This is comple-\nmented by a Multi-Kernel Positional Embedding (MKPE)\nencoder that captures spatial details across multiple recep-\ntive fields. Together, these components enable precise, effi-\ncient polyp segmentation in a unified encoder-decoder archi-\ntecture. We summarize our main contributions as follows:\n• Contour-Aware State-Space Modeling : We propose\nContour-Aware Mamba (CAM)—the first state-space\nmodel to explicitly incorporate contour sensitivity into\nits sequence modeling. By injecting learnable boundary\ncues via Learnable Contour Extractor into Mamba’s\nselective scan mechanism, CAM enables morphology-"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 2,
"chunk_index": 0,
"token_count": 512,
"text": "aware state transitions, enhancing spatial localization\nwithout sacrificing global context modeling. This design\nallows Mamba to capture long-range dependencies while\nremaining sensitive to fine boundary structures, a critical\naspect in medical image segmentation.\n• Unified Global-Local Architecture Design : We intro-\nduce CAM-PoEm, a novel encoder-decoder framework\nthat strategically combines local and global modeling\nwith boundary precision. MKPE encoders enhance spa-\ntial encoding by capturing multi-scale local structures,\nimproving robustness across varying polyp sizes and tex-\ntures. CAM is positioned in the bottleneck, where se-\nmantic abstraction is highest and spatial resolution is\nlowest—making it the ideal location to inject contour\nawareness into global reasoning. This placement en-\nables efficient morphology-aware context modeling over\ncompact feature representations, while keeping compu-\ntational cost low.\n• Comprehensive Empirical Validation across datasets:\nWe conduct extensive experiments on four bench-\nmark datasets—Kvasir-SEG (Jha et al. 2019a), Polyp-\nGen2021 (Ali et al. 2021), CVC-ClinicDB (Bernal et al.\n2015), and CVC-ColonDB (Bernal, S ´anchez, and Vi-\nlari˜no 2012). CAM-PoEm achieves state-of-the-art per-\nformance across all datasets, matching or outperforming\nexisiting segmentation baselines, particularly in bound-\nary precision and global consistency, while maintaining\nhigh computational efficiency.\nRelated Work\nConvolutional and Transformer-Based\nArchitectures\nCNN-based models such as U-Net (Ronneberger, Fischer,\nand Brox 2015b), FCNs (Long, Shelhamer, and Darrell\n2014), and ResUNet++ (Jha et al. 2019c) established strong\nbaselines for medical segmentation through local feature ex-\ntraction and skip connections. Extensions like A-DenseUNet\n(Safarov and Whangbo 2021) and Dilated U-Net (Karthikha,\nJamal, and Rafiammal 2024) improved multi-scale context\nvia atrous and dilated convolutions. However, CNNs inher-\nently struggle with capturing long-range dependencies due\nto their limited receptive fields and deep-layer inefficiency.\nTransformers addressed this via global self-attention, as\nseen in ViT (Dosovitskiy et al. 2021), Trans"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 2,
"chunk_index": 1,
"token_count": 512,
"text": " atrous and dilated convolutions. However, CNNs inher-\nently struggle with capturing long-range dependencies due\nto their limited receptive fields and deep-layer inefficiency.\nTransformers addressed this via global self-attention, as\nseen in ViT (Dosovitskiy et al. 2021), TransUNet (Chen\net al. 2021), and polyp-specific models like PraNet (Fan\net al. 2020a), ColonFormer (Duc et al. 2022), and Polyp-\nPVT (Dong et al. 2023). Despite improved boundary lo-\ncalization, transformers suffer from high memory cost and\nrequire large-scale pretraining, limiting their deployment in\nmedical contexts (Vaswani et al. 2017; Kirillov et al. 2023).\nHybrid models like SwinE-Net (Park and Lee 2022), Fo-\ncus U-Net (Yeung et al. 2021), and DCATNet (Wang et al.\n2025) combine CNNs for local encoding with transformers\nfor global reasoning. These improve performance but intro-\nduce significant complexity and often underperform at fine\nboundary delineation.\nMamba-Based Architectures\nMamba (Gu and Dao 2024) and Vision Mamba (Zhu\net al. 2024) introduced selective scanning for linear-time\nsequence modeling. Vision adaptations like Mamba-UNet\n(Wang et al. 2024c), VM-UNetV2 (Zhang et al. 2024),\nand ProMamba (Xie et al. 2024b) showed strong segmenta-\ntion capabilities via bidirectional and multi-scale extensions.\nMore recent efforts such as RM-UNet (Tang et al. 2024),\nLKM-UNet (Wang et al. 2024a), and Topo-VM-UNetV2\n(Adame et al. 2025) explored recurrence directionality and\nspatial topology. Nevertheless, these models lack contour\nguidance mechanisms, limiting their boundary precision in\nclinical segmentation.\nMulti-Kernel Positional Encoding (MKPE)\nMKPE enhances spatial awareness through multi-scale re-\nceptive fields, as in ConvNeXt-MPE (Mau et al. 2023),\nPEFNet, and MEP (Gao 2024). MKSA-BAHA (Zhou et al.\n202"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 2,
"chunk_index": 2,
"token_count": 499,
"text": "ernel Positional Encoding (MKPE)\nMKPE enhances spatial awareness through multi-scale re-\nceptive fields, as in ConvNeXt-MPE (Mau et al. 2023),\nPEFNet, and MEP (Gao 2024). MKSA-BAHA (Zhou et al.\n2024) extended this to city-scale segmentation via hybrid at-\ntention. Yet, many MKPE variants lack adaptability across\nresolutions and remain computationally expensive for high-\nresolution medical images.\nContour-Aware Modeling\nBoundary-guided models such as BDG-Net (Qiu et al.\n2022), Polyper (Shao, Zhang, and Hou 2023), and contour-\nconsistency decoders (Li et al. 2024b) explicitly encode\nedge priors via auxiliary branches or post-processing. While\neffective in improving contour alignment, they often rely on\nexternal supervision and are not fully integrated into the core\nmodeling pipeline. Recent efforts like LiteMamba-Bound\n(Ho et al. 2025b) aim for lightweight boundary-aware adap-\ntation but still lack unified, end-to-end design.\nMethodology\nThis section presents the proposedCAM-PoEm framework,\nwhich introduces Contour-Aware Mamba-based state-space\nmodeling within an encoder-decoder segmentation pipeline.\nOur motivation stems from the unique challenges in medical\nimage segmentation, especially in gastrointestinal polyp de-\nlineation, where ambiguous boundaries, diverse morpholo-\ngies, and local textural variations require both long-range\ncontext modeling and local edge sensitivity. We begin by\noutlining the overall architecture, followed by an in-depth\ndescription of each core component, including theoretical\nunderpinnings and implementation details.\nArchitectural Overview of CAM-PoEm\nCAM-PoEm adopts a U-shaped encoder-decoder frame-\nwork, augmented with a Mamba-based bottleneck for global\ncontext modeling. Let X ∈ RB×3×H×W denote the input\nmedical image, where B is the batch size, and H and W\nare spatial dimensions. The network outputs a segmentation\nmap Y ∈ RB×C×H×W , where C is the number of classes\n(typically C = 2 for binary segmentation). Fig. 1 shows the\narchitectural overview.\nThe encoder path consists of three DoubleCon-\nvWithMKPE blocks with channel dimensions 64, 128,"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 3,
"chunk_index": 0,
"token_count": 431,
"text": "Figure 1: The U-shaped architecture of the proposed model\nand 256, each followed by max-pooling to halve spatial di-\nmensions. The bottleneck employs Contour-Aware Mamba\nBlocks with a feature dimension dmodel = 128 , enabling\nefficient sequence modeling. The decoder path upsamples\nfeatures via transposed convolutions, concatenates skip\nconnections from the encoder, and applies DoubleCon-\nvWithMKPE blocks to refine features. Deep supervision\ngenerates auxiliary outputs at intermediate decoder stages,\nenhancing gradient flow. The final output is processed\nthrough a 1x1 convolution and an MKPE module to produce\nthe segmentation map.\nThe overall architecture can be mathematically formu-\nlated as:\nY = fM KP E(fD(fM(fE(X)))) (1)\nwhere fE(·), fM(·), fD(·), and fM KP E(·) represent the en-\ncoder, Mamba-based bottleneck, decoder, and final MKPE\noperations, respectively.\nProposed Multi-Kernel Positional Encoding\n(MKPE) for Local Spatial Awareness\nAccurate polyp segmentation requires capturing fine-\ngrained spatial cues and structural variations at multiple\nscales. Traditional positional encodings, such as sinusoidal\nFigure 2: MKPE module architecture. Multiple convolu-\ntional branches extract features at different scales, followed\nby concatenation and attention-based modulation. “⊕” indi-\ncates addition; “⊗” denotes element-wise multiplication.\nembeddings or fixed grid coordinates, often fail to represent\nsuch spatial intricacies—especially in medical images with\nvarying morphology and texture.\nTo address this, we propose a Multi-Kernel Posi-\ntional Encoding (MKPE) module, as shown in Figure. 2\nwhich enhances spatial representation through two mech-\nanisms: multi-scale convolutional encoding and attention-\nbased modulation.\nMulti-Scale Context Aggregation. Given an input fea-\nture map F ∈ RB×C×H×W , MKPE applies parallel depth-"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 4,
"chunk_index": 0,
"token_count": 512,
"text": "wise convolutional branches with kernel sizes 3 × 3, 5 × 5,\nand 7 × 7. These branches capture spatial patterns at vary-\ning receptive fields, enabling both local detail sensitivity and\nbroader contextual encoding. The outputs are concatenated\nalong the channel axis to form a unified representation:\nFmulti = Concat[Conv 3×3(F), Conv 5×5(F), Conv 7×7(F)]\nAttention-Based Spatial Recalibration. To adaptively\nemphasize spatially informative regions, MKPE employs a\nlightweight attention block. A series of 1 × 1 convolutions\nwith batch normalization and ReLU activation compress and\nrecalibrate the aggregated features:\nApos = σ\n\u0000\nfconv2\n\u0000\nReLU (BN (fconv1(Fmulti)))\n\u0001\u0001\nwhere Apos ∈ RB×C×H×W is the learned spatial attention\nmap and σ denotes the sigmoid function.\nResidual Feature Enhancement. The input is modulated\nby the attention map via residual fusion:\nFenhanced = F ⊙ Apos + F\nThis operation preserves the semantic richness of the origi-\nnal features while injecting multi-scale spatial priors into the\nrepresentation.\nIntegration with Encoder Blocks. Each encoder stage\nin CAM-PoEm adopts a DoubleConvWithMKPE block,\nwhere standard convolutional layers are augmented with\nMKPE. This ensures that early-stage representations encode\nstrong local positional awareness—critical for accurately\nidentifying polyp boundaries and textures.\nOverall, MKPE enables the encoder to focus on spatial\nsaliency across multiple scales, providing strong inductive\nbias for structure-preserving segmentation.\nProposed Learnable Contour Extractor\nSegmentation performance heavily relies on precise bound-\nary localization, which becomes especially difficult in med-\nical images with fuzzy or low-contrast edges. We address\nthis by introducing a Learnable Contour Extractor module,\nwhich builds on classical edge detection and enhances it\nwith deep feature learning.\nWe first apply Sobel filters along horizontal and vertical\ndirections to compute intensity gradients:\nGx = Sx ∗ F, Gy = Sy ∗ F\nwhere Sx and Sy are standard 3 × 3 Sobel kernels, and ∗\ndenotes convolution. These gradients emphasize abrupt in-\ntensity transitions that often align with object boundaries.\nNext, we compute the magnitude:\nGmag =\nq\nG2x + G2y + ϵ, �"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 4,
"chunk_index": 1,
"token_count": 512,
"text": "where Sx and Sy are standard 3 × 3 Sobel kernels, and ∗\ndenotes convolution. These gradients emphasize abrupt in-\ntensity transitions that often align with object boundaries.\nNext, we compute the magnitude:\nGmag =\nq\nG2x + G2y + ϵ, ϵ = 10−8 (2)\nTo improve robustness and learn task-relevant boundaries,\nGmag is refined through a lightweight enhancement net-\nwork composed of depthwise separable convolutions (for\nparameter efficiency) and a 1 × 1 projection:\nCf eat = Conv 1×1(σ(Conv 3×3(ReLU(BN (Gmag)))))\n(3)\nThe resulting contour prior Cf eat ∈ RB×H×W ×D is\naligned with Mamba’s latent dimension and used to inject\nboundary awareness into state-space modeling.\nFigure 3: Detailed block diagram of Contour-Aware Mamba\nBlock showing all components and data flow.\nAlgorithm 1: Contour-Aware Mamba Block (CAM-Block)\nRequire: Input sequence x ∈ RB×L×D\n1: Project input: [xssm, xres] ← Linear2Dinner(x)\n2: Apply depthwise conv: xconv ←\nSiLU (Conv1Ddw(xssm))\n3: Compute SSM params: [δ, B, C] ←\nSplit(Linear(xconv))\n4: Smooth decay: δ ← Sof tplus(Linear(δ))\n5: Extract contour prior: Cf eatures ←\nLayerN orm(Linear(ContourExtractor (x)))\n6: Contour-guided scan: yssm ←\nContourGuidedScan (xconv, δ, A, B, C, D, Cf eatures)\n7: Channel attention:\n8: yatt ← yssm⊙σ(ff c2(ReLU(ff c1(GAP (yssm)))))\n9: Fuse project: y ← Dropout(LinearD(yatt ⊙\nSiLU (xres)))\n10: return y\nProposed Contour-Aware Mamba (CAM)\nBottleneck\nMamba with linear scan is a state-space model that captures\nlong-range dependencies in sequences via selective scanning\nwith linear time complexity. However, it lacks spatial in-\nductive bias, treating all tokens uniformly—limiting its util-\nity in segmentation. We propose the Contour-Aware Mamba\n(CAM) block, which injects"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 4,
"chunk_index": 2,
"token_count": 181,
"text": " is a state-space model that captures\nlong-range dependencies in sequences via selective scanning\nwith linear time complexity. However, it lacks spatial in-\nductive bias, treating all tokens uniformly—limiting its util-\nity in segmentation. We propose the Contour-Aware Mamba\n(CAM) block, which injects anatomical boundary priors to\nguide selective scanning. Figure 3 shows the detailed dia-\ngram.\nThe standard Mamba recurrence is:\nht = ¯Atht−1 + ¯Btut (4)\nyt = Ctht + Dut (5)\nwhere ut is the input token at time t, and ¯At, ¯Bt, Ct, D\nare learnable matrices governing the state transitions and\noutput projection.\nAlgorithm 1 outlines the forward pass of the Contour-\nAware Mamba Block (CAM-Block), which integrates con-"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 5,
"chunk_index": 0,
"token_count": 512,
"text": "tour structure into the selective scanning mechanism of a\nstate-space model (SSM).\nLocal context extraction. Lines 1–2 split the input se-\nquence into two branches—one for SSM computation and\none for residual fusion. The SSM branch uses depthwise\nseparable convolution, which preserves local structure with\nfar fewer parameters than standard 1D conv, improving ef-\nficiency without sacrificing performance. SiLU activation\nadds non-linearity.\nSSM parameter generation. Lines 3–4 extract decay pa-\nrameters δ and transformation matrices B, C using linear\nlayers. A Softplus ensures positivity in decay dynamics,\nenhancing stability during scanning.\nContour-aware enhancement. Line 5 introduces the core\ninnovation: explicit contour conditioning. A lightweight\nContourExtractor (Equation 3) module captures\nboundary information from the original input x. Contour\nfeatures Cf eatures ∈ RB×L×D are reshaped to match the\nsequence dimension, and a contour weight is computed:\nWc = σ(Cf eat) (6)\nSelective scanning. In Line 6, we perform\nContourGuidedScan, a modified SSM scan, where\nWc modulates the scan interval ∆t, effectively increasing\ndwell time near edges. The scan specially incorporates Wc,\ninto the the output projection matrix Ct only, leaving the\ninternal state dynamics intact. The modulation is applied as:\nCmod\nt = Ct ⊙ (1 + α · Cf eatures) (7)\nwhere α is a learnable scalar controlling the strength of\ncontour influence, and ⊙ denotes element-wise multiplica-\ntion. This ensures edge-aware semantics are emphasized in\nthe readout phase while preserving internal recurrence via\n¯At, ¯Bt. We modulate only Ct to avoid disrupting the stable\nand efficient dynamics of Mamba’s core SSM.\nChannel-wise recalibration. Lines 7–8 apply squeeze-\nand-excitation (SE) attention to enhance informative chan-\nnels. This is lightweight due to reduced inner dimensionality\nand focuses on high-salience semantic content.\nResidual fusion. Line 9 projects the attended output and\nfuses it with the residual stream through element-wise mul-\ntiplication. This adds robustness while maintaining gradient\nflow and semantic integrity.\nDeep Supervision and Final Prediction\nTo enhance learning and ensure better gradient propagation,\nwe apply deep supervision across multiple decoder stages.\nIntermediate segmentation maps are upsampled and com"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 5,
"chunk_index": 1,
"token_count": 512,
"text": " and\nfuses it with the residual stream through element-wise mul-\ntiplication. This adds robustness while maintaining gradient\nflow and semantic integrity.\nDeep Supervision and Final Prediction\nTo enhance learning and ensure better gradient propagation,\nwe apply deep supervision across multiple decoder stages.\nIntermediate segmentation maps are upsampled and com-\npared with ground truth using a combined Dice and Binary\nCross Entropy (BCE) loss. The overall loss is given by:\nLtotal =\nX\ni\nwi(Li\nDice + Li\nBCE ) (8)\nwhere wi denotes the weighting coefficient for each stage.\nExperiments\nDatasets\nWe evaluate CAM-PoEm on four widely-used polyp\nsegmentation benchmarks. Kvasir-SEG (Jha et al.\n2019a)consists of 1,000 high-quality images with cor-\nresponding polyp masks. CVC-ClinicDB (Bernal et al.\n2015) contains 612 clinical images with expert-annotated\npolyp masks. CVC-ColonDB (Bernal, S ´anchez, and Vi-\nlari˜no 2012) provides 380 low-resolution polyp images\nand masks, representing challenging real-world scenar-\nios. Finally, PolypGen21 (Ali et al. 2021) includes 807\nimages with masks covering polyps, normal tissue, and\nfalse-positive regions, facilitating evaluation under realistic\nclinical variability.\nImplementation Details\nThe model was implemented in PyTorch and trained on\nNVIDIA GPUs using mixed precision for computational ef-\nficiency. Training utilized a combined loss function integrat-\ning Dice and Focal losses, weighted at 0.7 and 0.3 respec-\ntively, to effectively handle class imbalance. Optimization\nwas done using the AdamW optimizer with an initial learn-\ning rate of 1.8e-4, alongside a cosine annealing scheduler\nand a 15-epoch warm-up period. Input images were resized\nto 352×352, and training was performed with a batch size\nof 16 for up to 200 epochs with early stopping patience of\n30 epochs. The architecture includes 6 sequential Mamba\nblocks with a model dimension of 128 and a dropout rate of\n0.2.\nResults and Analysis\nPerformance of CAM-PoEM Across Datasets. We eval-\nuated our proposed CAM-PoEM model on four widely\nused polyp segmentation benchmarks: Kvas"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 5,
"chunk_index": 2,
"token_count": 397,
"text": " The architecture includes 6 sequential Mamba\nblocks with a model dimension of 128 and a dropout rate of\n0.2.\nResults and Analysis\nPerformance of CAM-PoEM Across Datasets. We eval-\nuated our proposed CAM-PoEM model on four widely\nused polyp segmentation benchmarks: Kvasir, ClinicDB,\nColonDB, and PolypGen. Our model achieves consistent\nand competitive performance across all datasets with a\nlightweight architecture of only 4.96M parameters and\n51.10 GFLOPs. Specifically, CAM-PoEM achieves a mean\nDice score of 91.65% and mIoU of 85.35% on Kvasir,\nand 91.50% Dice and 85.89% IoU on ClinicDB, demon-\nstrating strong generalization on clean and high-resolution\ndatasets. On the more challenging ColonDB and PolypGen\ndatasets—which contain low-quality, diverse, and complex\nsamples—our model maintains high accuracy, with Dice\nscores of 88.31% and 88.27%, and IoUs of 81.83% and\n83.20%, respectively. These results highlight CAM-PoEM’s\nrobustness in capturing both global context and fine-grained\nboundaries across varying polyp morphologies.\nQuantitative Comparison with State-of-the-Art Meth-\nods. We compare CAM-PoEM with recent state-of-the-art\nsegmentation models in Table 1. Our model achieves top-\ntier performance while being significantly more efficient in\nterms of parameter count and computational cost. Notably,\nCAM-PoEM outperforms larger models such as UACANet-\nL (69.16M) and SSFormer-L (66.22M) on ColonDB and\nPolypGen in terms of Dice and IoU, despite having an or-\nder of magnitude fewer parameters. On ColonDB, our model"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 6,
"chunk_index": 0,
"token_count": 512,
"text": "Methods Params (M)FLOPs (G) Kvasir ClinicDB ColonDB PolypGen\nmDice mIoU mDice mIoU mDice mIoU mDice mIoU\nUNet (Ronneberger, Fischer, and Brox 2015c)34.53 65.53G 81.80 74.60 82.30 75.50 51.20 44.40 - -\nUNet++ (Zhou et al. 2018) 25.09 84.30 82.10 74.30 79.40 72.90 48.30 41.00 - -\nAttnUNet (Oktay et al. 2018) 34.88 66.64 83.49 76.84 86.46 79.66 52.33 45.10 - -\nDeepLabv3+ (Chen et al. 2018) 39.76 14.92 89.06 83.72 91.24 84.91 65.32 57.85 - -\nPraNet (Fan et al. 2020b) 32.55 221.90 89.80 84.00 89.90 84.90 70.90 64.00 - -\nCaraNet (Lou, Guan, and Loew 2023) 46.64 11.48 89.75 83.25 91.70 85.34 65.55 57.72 - -\nUACANet-L (Kim, Lee, and Kim 2021) 69.16 31.51 90.02 85.25 91.02 84.76 69.81 63.10 - -\nACSNet (Liu et al. 2021) 46.02 29.45 89.80 83.80 88.20 82.60 71.60 64.90 - -\nSFA (Luo et al. 2019) - - 72.30 61.10 70.00 60.70 46.90 34.70 - -\nYolo-SAM2 (Mansoori et al. 2024) - - 86.60 76.40 - - - - 80.80 67.80\n"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 6,
"chunk_index": 1,
"token_count": 512,
"text": ".30 61.10 70.00 60.70 46.90 34.70 - -\nYolo-SAM2 (Mansoori et al. 2024) - - 86.60 76.40 - - - - 80.80 67.80\nLGPS (Tesema et al. 2025) – – - - - - - - 72.99 78.67\nCAM-PoEM (Ours) 4.96 51.10 91.65 85.35 91.50 85.89 88.31 81.83 88.27 83.20\nTable 1: Quantitative comparison of CAM-PoEM with state-of-the-art models on four benchmark datasets: Kvasir, ClinicDB,\nColonDB, and PolypGen. The reported results for baseline methods are sourced from their respective papers or public bench-\nmarks and are included solely for reference, as variations in dataset splits, preprocessing pipelines, or image resolutions may\nexist. A dash (’–’) denotes that the corresponding result was not available or not reported in the source.\nachieves the highest Dice (88.31%) and IoU (81.83%), and\nperforms on par with leading models on other datasets.\nQualitative and Failure Case Analysis\nQualitative Analysis. We visualize segmentation results\nin Figure 4 to qualitatively compare CAM-PoEm with\nstate-of-the-art baselines, including UNet, Attention-UNet,\nand SwinUNet. Evaluation is performed on representative\nsamples from the Kvasir and PolypGen datasets. CAM-\nPoEm consistently produces crisp and accurate contours,\neffectively capturing both small and multiple polyps per\nframe—especially in Kvasir. In PolypGen, our model adapts\nrobustly to variations in lighting, tissue texture, and polyp\nmorphology, outperforming others in cases with low contrast\nor irregular shapes. Baseline models tend to under-segment\nor overlook faint boundaries, particularly in multi-polyp or\ncluttered backgrounds. These results underscore the strength\nof our boundary-sensitive global modeling under real-world\nclinical variations.\nFailure Case Analysis. We further examined cases\nfrom the CVC-ClinicDB dataset to evaluate potential fail-\nure modes (Figure 5). Unlike Kvasir or PolypGen, sev-\neral ground truth masks in ClinicDB were either overly"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 6,
"chunk_index": 2,
"token_count": 512,
"text": " our boundary-sensitive global modeling under real-world\nclinical variations.\nFailure Case Analysis. We further examined cases\nfrom the CVC-ClinicDB dataset to evaluate potential fail-\nure modes (Figure 5). Unlike Kvasir or PolypGen, sev-\neral ground truth masks in ClinicDB were either overly\ncoarse or poorly aligned with visible polyp boundaries.\nIn such scenarios, CAM-PoEm’s output—although visu-\nally and anatomically plausible—exhibited reduced overlap\nwith the annotated ground truths, leading to lower Dice or\nmIoU scores. This mismatch highlights a key limitation:\nhigh structural sensitivity may conflict with sub-optimal la-\nbel quality, resulting in underreported performance despite\nclinically valid predictions. Such observations suggest the\nneed for uncertainty-aware or soft-label evaluation in future\npolyp segmentation benchmarks.\nAblation Study\nWe conduct comprehensive ablation studies to evaluate the\nindividual and combined contributions of the two key com-\nponents of CAM-PoEm: (i) the Contour-Aware Mamba\n(CAM) bottleneck, and (ii) the Multi-Kernel Positional\nEmbedding (MKPE) encoder. Experiments are conducted\non the Kvasir-SEG dataset under consistent training set-\ntings, and performance is reported as mean Intersection over\nUnion (mIoU) and Dice coefficient (mDice) across three\nruns, along with standard deviation (SD).\nAblation of Contour-Aware Mamba Bottleneck To as-\nsess the effectiveness of our proposed CAM bottleneck,\nwe compare the segmentation performance of models with\nthe standard Mamba block versus the contour-aware vari-\nant. As shown in Table 2, introducing contour cues into\nthe Mamba scan mechanism enhances boundary sensitiv-\nity, leading to better segmentation results—especially when\ncombined with MKPE. While CAM slightly performs better\nthan standard Mamba in isolation, it yields notable improve-\nment in the final model, confirming that contour-guided\nglobal modeling complements local spatial features.\nAblation of Multi-Kernel Positional Embedding\n(MKPE) Encoder We evaluate the contribution of\nMKPE by replacing the standard CNN encoder with our\nproposed multi-kernel positional embedding module. As\npresented in Table 3, MKPE substantially improves spatial\nrepresentation and performance when used in conjunction\nwith both standard Mamba and CAM bottlenecks. This\nhighlights the value of multi-scale context modeling at\nthe encoder stage, which captures"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 6,
"chunk_index": 3,
"token_count": 222,
"text": " our\nproposed multi-kernel positional embedding module. As\npresented in Table 3, MKPE substantially improves spatial\nrepresentation and performance when used in conjunction\nwith both standard Mamba and CAM bottlenecks. This\nhighlights the value of multi-scale context modeling at\nthe encoder stage, which captures heterogeneous polyp\nstructures more effectively.\nAblation on making different mamba parameters\nContour Aware\nThe Mamba architecture includes four learnable parameters:\nA, B, C, and D. In our Contour-Aware Mamba (CAM) de-\nsign, we selectively introduce inductive bias only in the C\nparameter, as it directly influences the input-to-state projec-\ntion and spatial transformation. To evaluate the impact of\nmodifying different parameters, we conducted an ablation\nby integrating contour-awareness into various combinations.\nWhy only C? The C parameter projects spatial features\nfrom input to state space. By incorporating contour priors\ninto this mapping, the model can better preserve structural\nboundaries without over-parameterizing. In contrast, modi-"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 7,
"chunk_index": 0,
"token_count": 512,
"text": "Figure 4: Qualitative comparison of CAM-PoEm against UNet, Attention-UNet, and SwinUNet on challenging samples from\nKvasir and PolypGen. CAM-PoEm achieves sharper contours, detects small polyps, and maintains segmentation fidelity under\nvisual ambiguity.\nModel Variant Dice Gain\nCNN + Mamba 0.8873 ± 0.0044 Reference\nCNN + CAM 0.8891 ± 0.0037 +0.2%\nMKPE + Mamba 0.8834 ± 0.0039 —\nMKPE + CAM (Ours) 0.9165 ± 0.0035 +3.3%\nTable 2: Ablation study highlighting the contribution of\nthe proposed Contour-Aware Mamba (CAM) . CAM en-\nhances boundary-sensitive global modeling and consistently\nimproves performance, particularly when combined with the\nMKPE encoder.\nModel Variant Dice Gain\nCNN + Mamba 0.8873 ± 0.0044 Reference\nMKPE + Mamba 0.8834 ± 0.0039 -0.4%\nCNN + CAM 0.8851 ± 0.0037 —\nMKPE + CAM (Ours) 0.9165 ± 0.0035 +2.2%\nTable 3: Ablation study highlighting the contribution of the\nMulti-Kernel Positional Embedding (MKPE). MKPE en-\nhances multi-scale local representation and synergizes effec-\ntively with CAM to achieve superior segmentation perfor-\nmance.\nFigure 5: Failure case from CVC-ClinicDB. CAM-PoEm\nproduces clinically reasonable segmentation masks, but\ndiverges from ground truth due to imprecise annota-\ntions—highlighting dataset-dependent limitations.\nModified Params mIoU mDice Total Epochs\nCAM-A (A only) 0.8257 0.8929 120\nCAM-B (B only) 0.8100 0.8819 93\nCAM-A-B-D 0.8122 0.8817 101\nCAM-C (Ours) 0.8535 0.9161 –\nCAM-C+D 0.8320 0.8900 –\nTable 4: Ablation on modifying different Mamba parameters\nwith contour-aware bias. CAM-C (modifying onlyC) yields"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 7,
"chunk_index": 1,
"token_count": 512,
"text": "101\nCAM-C (Ours) 0.8535 0.9161 –\nCAM-C+D 0.8320 0.8900 –\nTable 4: Ablation on modifying different Mamba parameters\nwith contour-aware bias. CAM-C (modifying onlyC) yields\nthe best results.\nfying other parameters such as A (state transition), B (input\nmixing), or D (output projection) either introduces noise or\nresults in diminished boundary sensitivity. Combining mul-\ntiple parameters (CAM-A-B-D or CAM-C+D) adds unnec-\nessary complexity and leads to lower performance. These\nfindings validate our design choice of isolating the induc-\ntive bias to only C, ensuring both efficiency and accuracy in\nboundary-sensitive segmentation tasks.\nDiscussion\nGlobal Modeling with Bottleneck-Positioned CAM.\nCAM-PoEm demonstrates that injecting learnable contour\npriors into state-space models (SSMs) enables boundary-\nsensitive sequence modeling without compromising global\ncontext. By placing the Contour-Aware Mamba (CAM)\nblock in the bottleneck—where spatial resolution is reduced\nbut semantic abstraction is highest—the model leverages\nlow-dimensional, high-level features for global reasoning,\noptimizing both accuracy and efficiency. This design aligns\nwith encoder-decoder principles where context modeling at\nthe bottleneck yields maximal receptive field coverage at\nminimal computational cost. The CAM block modulates\nMamba’s output projection with contour priors while pre-\nserving its stable recurrent dynamics, making it suitable for\nmemory-constrained clinical segmentation tasks.\nContour-Aware Mamba via Linear Token Scanning.\nThe introduction of contour-awareness into Mamba’s linear-\ntime selective scan addresses a core limitation of existing\nSSM-based vision models: the uniform treatment of spatial\ntokens. CAM-PoEm injects edge-weighted priors into the\nprojection matrix Ct, enabling the model to allocate greater\nemphasis to boundary-adjacent regions during output gener-\nation. This preserves Mamba’s O(N) complexity while en-\nabling morphology-aware token processing—a crucial im-\nprovement for segmenting ambiguous or irregular lesions in\nmedical imaging.\nConclusion\nCombined with the Multi-Kernel Positional Embedding\n(MKPE) encoder, which captures local structure at multiple\nreceptive fields, CAM-PoEm achieves state-of-the-art seg-\nmentation performance across four polyp benchmarks using\nonly 4.96M parameters. Ablation"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 7,
"chunk_index": 2,
"token_count": 90,
"text": "medical imaging.\nConclusion\nCombined with the Multi-Kernel Positional Embedding\n(MKPE) encoder, which captures local structure at multiple\nreceptive fields, CAM-PoEm achieves state-of-the-art seg-\nmentation performance across four polyp benchmarks using\nonly 4.96M parameters. Ablation studies confirm the syn-\nergistic effect between CAM and MKPE, delivering both\nglobal coherence and fine-grained boundary localization."
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 8,
"chunk_index": 0,
"token_count": 512,
"text": "References\nAdame, D.; Nunez, J. A.; Vazquez, F.; Gurrola, N.; Li, H.;\nTang, H.; Fu, B.; and Gu, P. 2025. Topo-VM-UNetV2: En-\ncoding Topology into Vision Mamba UNet for Polyp Seg-\nmentation.\nAli, S.; Jha, D.; Ghatwary, N.; Realdon, S.; Cannizzaro, R.;\nSalem, O.; Lamarque, D.; Daul, C.; ˚Anonsen, K. V .; Riegler,\nM.; Halvorsen, P.; Rittscher, J.; de Lange, T.; and East, J.\n2021. PolypGen: A Multi-Center Polyp Detection and Seg-\nmentation Dataset for Generalisability Assessment. Multi-\ncenter endoscopic polyp dataset with 1,000+ images from 6\nmedical centers, arXiv:2106.04463.\nBernal, J.; S´anchez, F. J.; Fern´andez-Esparrach, G.; Gil, D.;\nRodr´ıguez, C.; and Vilari ˜no, F. 2015. WM-DOV A maps\nfor accurate polyp highlighting in colonoscopy: Validation\nvs. saliency maps from physicians. Computerized Medical\nImaging and Graphics, 43: 99–111. Official journal of the\nComputerized Medical Imaging Society.\nBernal, J.; S ´anchez, J.; and Vilari ˜no, F. 2012. Towards\nautomatic polyp detection with a polyp appearance model.\nPattern Recognition , 45(9): 3166–3182. Best Papers of\nIberian Conference on Pattern Recognition and Image Anal-\nysis (IbPRIA’2011).\nCao, H.; Wang, Y .; Chen, J.; Jiang, D.; Zhang, X.; Tian, Q.;\nand Wang, M. 2021. Swin-Unet: Unet-like Pure Transformer\nfor Medical Image Segmentation. arXiv:2105.05537.\nChen, J.; Lu, Y .; Yu, Q.; Luo, X.; Adeli, E.; Wang, Y .; Lu,\nL.; Yuille, A. L.; and Zhou, Y ."
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 8,
"chunk_index": 1,
"token_count": 512,
"text": "-like Pure Transformer\nfor Medical Image Segmentation. arXiv:2105.05537.\nChen, J.; Lu, Y .; Yu, Q.; Luo, X.; Adeli, E.; Wang, Y .; Lu,\nL.; Yuille, A. L.; and Zhou, Y . 2021. TransUNet: Transform-\ners Make Strong Encoders for Medical Image Segmentation.\narXiv:2102.04306.\nChen, L.-C.; Zhu, Y .; Papandreou, G.; Schroff, F.; and\nAdam, H. 2018. Encoder-Decoder with Atrous Sep-\narable Convolution for Semantic Image Segmentation.\narXiv:1802.02611.\nDjinbachian, R.; Iratni, R.; Durand, M.; Marques, P.; and\nvon Renteln, D. 2020. Rates of Incomplete Resection of\n1- to 20-mm Colorectal Polyps: A Systematic Review and\nMeta-Analysis. Gastroenterology, 159(3): 904–914.e12.\nDong, B.; Wang, W.; Fan, D.-P.; Li, J.; Fu, H.; and Shao,\nL. 2023. Polyp-PVT: Polyp Segmentation with Pyramid Vi-\nsion Transformers. CAAI Artificial Intelligence Research ,\n9150015. Article ID: 9150015.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. Published as a conference paper at\nICLR 2021, arXiv:2010.11929.\nDuc, N. T.; Oanh, N. T.; Thuy, N. T.; Triet, T. M.; and Dinh,\nV . S. 2022. ColonFormer: An Efficient Transformer Based\nMethod for Colon Polyp Segmentation. IEEE Access, 10:\n80575–80586.\nFan, D.-P.; Ji, G.-P.; Zhou, T.;"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 8,
"chunk_index": 2,
"token_count": 512,
"text": ".; Triet, T. M.; and Dinh,\nV . S. 2022. ColonFormer: An Efficient Transformer Based\nMethod for Colon Polyp Segmentation. IEEE Access, 10:\n80575–80586.\nFan, D.-P.; Ji, G.-P.; Zhou, T.; Chen, G.; Fu, H.; Shen, J.;\nand Shao, L. 2020a. PraNet: Parallel Reverse Attention Net-\nwork for Polyp Segmentation. Published in MICCAI 2020,\narXiv:2006.11392.\nFan, D.-P.; Ji, G.-P.; Zhou, T.; Chen, G.; Fu, H.; Shen, J.; and\nShao, L. 2020b. PraNet: Parallel Reverse Attention Network\nfor Polyp Segmentation. arXiv:2006.11392.\nGao, W. 2024. MEP: Multiple Kernel Learning Enhanc-\ning Relative Positional Encoding Length Extrapolation.\narXiv:2403.17698.\nGu, A.; and Dao, T. 2023. Mamba: Linear-Time Sequence\nModeling with Selective State Spaces. Published at ICLR\n2024, arXiv:2312.00752.\nGu, A.; and Dao, T. 2024. Mamba: Linear-Time Sequence\nModeling with Selective State Spaces.\nHaggar, F. A.; and Boushey, R. P. 2009. Colorectal cancer\nepidemiology: incidence, mortality, survival, and risk fac-\ntors. Clinics in colon and rectal surgery, 22(4): 191–197.\nHo, Q.-H.; Nguyen, T.-N.-Q.; Tran, T.-T.; and Pham, V .-\nT. 2025a. LiteMamba-Bound: A lightweight Mamba-based\nmodel with boundary-aware and normalized active contour\nloss for skin lesion segmentation. Methods, 235: 10–25.\nHo, Q.-H.; Nguyen, T.-N.-Q.; Tran, T.-T.; and Pham, V .-T.\n2025b. LiteMamba-Bound: A Lightweight Mamba-Based\nModel with Boundary-Aware and Normalized Active Con-\ntour Loss for Skin Lesion Segmentation. Methods, 235: 10–\n25.\nJha, D.; Smed"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 8,
"chunk_index": 3,
"token_count": 512,
"text": ".; and Pham, V .-T.\n2025b. LiteMamba-Bound: A Lightweight Mamba-Based\nModel with Boundary-Aware and Normalized Active Con-\ntour Loss for Skin Lesion Segmentation. Methods, 235: 10–\n25.\nJha, D.; Smedsrud, P. H.; Riegler, M. A.; Halvorsen, P.;\nde Lange, T.; Johansen, D.; and Johansen, H. D. 2019a.\nKvasir-SEG: A Segmented Polyp Dataset. Medical image\nsegmentation dataset, arXiv:1911.07069.\nJha, D.; Smedsrud, P. H.; Riegler, M. A.; Johansen, D.;\nde Lange, T.; Halvorsen, P.; and Johansen, H. D. 2019b.\nResUNet++: An Advanced Architecture for Medical Image\nSegmentation. arXiv:1911.07067.\nJha, D.; Smedsrud, P. H.; Riegler, M. A.; Johansen, D.;\nde Lange, T.; Halvorsen, P.; and Johansen, H. D. 2019c.\nResUNet++: An Advanced Architecture for Medical Im-\nage Segmentation. Published in IEEE Access 2021,\narXiv:1911.07067.\nKarthikha, R.; Jamal, D. N.; and Rafiammal, S. S. 2024.\nAn approach of polyp segmentation from colonoscopy im-\nages using Dilated-U-Net-Seg – A deep learning network.\nBiomedical Signal Processing and Control, 93: 106197.\nKim, T.; Lee, H.; and Kim, D. 2021. UACANet: Uncertainty\nAugmented Context Attention for Polyp Segmentation. In\nProceedings of the 29th ACM International Conference on\nMultimedia, 2167–2175. ACM.\nKirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.;\nGustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo, W.-\nY .; Doll´ar, P.; and Girshick, R. 2023. Segment Anything.\nMeta AI Research, arXiv"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 8,
"chunk_index": 4,
"token_count": 188,
"text": " H.; Rolland, C.;\nGustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo, W.-\nY .; Doll´ar, P.; and Girshick, R. 2023. Segment Anything.\nMeta AI Research, arXiv:2304.02643.\nLi, L.; Lian, S.; Luo, Z.; Wang, B.; and Li, S. 2024a.\nContour-aware consistency for semi-supervised medical im-\nage segmentation. Biomedical Signal Processing and Con-\ntrol, 89: 105694.\nLi, L.; Lian, S.; Luo, Z.; Wang, B.; and Li, S. 2024b.\nContour-Aware Consistency for Semi-Supervised Medical\nImage Segmentation. Biomedical Signal Processing and\nControl, 89: 105694."
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 9,
"chunk_index": 0,
"token_count": 512,
"text": "Li, W.; Xiong, X.; Li, S.; and Fan, F. 2024c. HybridVPS:\nHybrid-Supervised Video Polyp Segmentation Under Low-\nCost Labels. IEEE Signal Processing Letters, 31: 111–115.\nLiu, Y .; Zhang, Y .; Wang, Y .; Hou, F.; Yuan, J.; Tian, J.;\nZhang, Y .; Shi, Z.; Fan, J.; and He, Z. 2024. A Survey of Vi-\nsual Transformers. IEEE Transactions on Neural Networks\nand Learning Systems, 35(6): 7478–7498. Early Access.\nLiu, Z.; Wang, L.; Zhang, Q.; Tang, W.; Yuan, J.; Zheng, N.;\nand Hua, G. 2021. ACSNet: Action-Context Separation Net-\nwork for Weakly Supervised Temporal Action Localization.\narXiv:2103.15088.\nLong, J.; Shelhamer, E.; and Darrell, T. 2014. Fully Con-\nvolutional Networks for Semantic Segmentation. arXiv\npreprint, abs/1411.4038. Published in CVPR 2015.\nLong, J.; Shelhamer, E.; and Darrell, T. 2015. Fully\nConvolutional Networks for Semantic Segmentation.\narXiv:1411.4038.\nLou, A.; Guan, S.; and Loew, M. 2023. CaraNet: context ax-\nial reverse attention network for segmentation of small med-\nical objects. Journal of Medical Imaging, 10(01).\nLuo, S.; Li, X.; Zhu, R.; and Zhang, X. 2019. SFA:\nSmall Faces Attention Face Detector. IEEE Access , 7:\n171609–171620.\nMa, J.; Li, F.; and Wang, B. 2024. U-Mamba: Enhancing\nLong-range Dependency for Biomedical Image Segmenta-\ntion. arXiv:2401.04722.\nMansoori, M.; Shahabodini, S.; Abouei, J.; Plataniotis,\nK. N.; and Mohammadi, A. 2024. Self-Prompting Polyp\nSegmentation in Colonoscopy using Hybrid Yolo-SAM 2\nModel. arXiv:2409.094"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 9,
"chunk_index": 1,
"token_count": 512,
"text": "abodini, S.; Abouei, J.; Plataniotis,\nK. N.; and Mohammadi, A. 2024. Self-Prompting Polyp\nSegmentation in Colonoscopy using Hybrid Yolo-SAM 2\nModel. arXiv:2409.09484.\nMau, T.-H. N.; Trinh, Q.-H.; Bui, N.-T.; Tran, M.-T.; and\nNguyen, H.-D. 2023. Multi Kernel Positional Embedding\nConvNeXt for Polyp Segmentation. arXiv:2301.06673.\nOktay, O.; Schlemper, J.; Folgoc, L. L.; Lee, M.; Hein-\nrich, M.; Misawa, K.; Mori, K.; McDonagh, S.; Hammerla,\nN. Y .; Kainz, B.; Glocker, B.; and Rueckert, D. 2018. At-\ntention U-Net: Learning Where to Look for the Pancreas.\narXiv:1804.03999.\nPark, K.-B.; and Lee, J. Y . 2022. SwinE-Net: Hybrid Deep\nLearning Approach to Novel Polyp Segmentation Using\nConvolutional Neural Network and Swin Transformer.Jour-\nnal of Computational Design and Engineering , 9(2): 616–\n632.\nPooler, B. D.; Kim, D. H.; Matkowskyj, K. A.; Newton,\nM. A.; Halberg, R. B.; Grady, W. M.; Hassan, C.; and Pick-\nhardt, P. J. 2023. Growth rates and histopathological out-\ncomes of small (6-9 mm) colorectal polyps based on CT\ncolonography surveillance and endoscopic removal. Gut,\n72(12): 2321–2328.\nQiu, Z.; Wang, Z.; Zhang, M.; Xu, Z.; Fan, J.; and Xu, L.\n2022. BDG-Net: Boundary Distribution Guided Network\nfor Accurate Polyp Segmentation. In I ˇsgum, I.; and Colliot,\nO., eds., Medical Imaging 2022: Image Processing. SPIE.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015a. U-Net"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 9,
"chunk_index": 2,
"token_count": 512,
"text": "\nfor Accurate Polyp Segmentation. In I ˇsgum, I.; and Colliot,\nO., eds., Medical Imaging 2022: Image Processing. SPIE.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015a. U-Net:\nConvolutional Networks for Biomedical Image Segmenta-\ntion. arXiv:1505.04597.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015b. U-Net:\nConvolutional Networks for Biomedical Image Segmenta-\ntion. Published in MICCAI 2015, arXiv:1505.04597.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015c. U-Net:\nConvolutional Networks for Biomedical Image Segmenta-\ntion. arXiv:1505.04597.\nRuan, J.; Li, J.; and Xiang, S. 2024. VM-UNet: Vi-\nsion Mamba UNet for Medical Image Segmentation.\narXiv:2402.02491.\nSafarov, S.; and Whangbo, T. K. 2021. A-DenseUNet:\nAdaptive Densely Connected UNet for Polyp Segmentation\nin Colonoscopy Images with Atrous Convolution. Sensors,\n21(4): 1441. Article Number: 1441.\nShao, H.; Zhang, Y .; and Hou, Q. 2023. Polyper: Boundary\nSensitive Polyp Segmentation.\nTang, H.; Huang, G.; Cheng, L.; Yuan, X.; Tao, Q.; Chen,\nX.; Zhong, G.; and Yang, X. 2024. RM-UNet: UNet-like\nMamba with Rotational SSM Module for Medical Image\nSegmentation. Signal, Image and Video Processing , 18:\n8427–8443.\nTesema, F. B.; Manzanares, A. G.; Cui, T.; Zhang, Q.;\nSolomon, M.; and He, S. 2025. LGPS: A Lightweight GAN-\nBased Approach for Polyp Segmentation in Colonoscopy\nImages. arXiv:2503.18294.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 9,
"chunk_index": 3,
"token_count": 512,
"text": " He, S. 2025. LGPS: A Lightweight GAN-\nBased Approach for Polyp Segmentation in Colonoscopy\nImages. arXiv:2503.18294.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention Is All You Need. Published in NeurIPS 2017.\nWang, J.; Chen, J.; Chen, D.; and Wu, J. 2024a. LKM-UNet:\nLarge Kernel Vision Mamba UNet for Medical Image Seg-\nmentation. arXiv:2403.07332.\nWang, Z.; Li, T.; Liu, M.; Jiang, J.; and Liu, X. 2025. DCAT-\nNet: Polyp Segmentation with Deformable Convolution and\nContextual-Aware Attention Network. BMC Medical Imag-\ning, 25.\nWang, Z.; Zheng, J.-Q.; Zhang, Y .; Cui, G.; and Li, L. 2024b.\nMamba-UNet: UNet-Like Pure Visual Mamba for Medical\nImage Segmentation. arXiv:2402.05079.\nWang, Z.; Zheng, J.-Q.; Zhang, Y .; Cui, G.; and Li, L. 2024c.\nMamba-UNet: UNet-Like Pure Visual Mamba for Medical\nImage Segmentation.\nXie, J.; Liao, R.; Zhang, Z.; Yi, S.; Zhu, Y .; and Luo, G.\n2024a. ProMamba: Prompt-Mamba for polyp segmentation.\narXiv:2403.13660.\nXie, J.; Liao, R.; Zhang, Z.; Yi, S.; Zhu, Y .; and Luo, G.\n2024b. ProMamba: Prompt-Mamba for Polyp Segmenta-\ntion.\nXu, Z.; Tang, F.; Chen, Z.; Zhou, Z.; Wu, W.; Yang, Y .;\nLiang, Y .; Jiang, J.; Cai, X.; and Su, J. 2024. Polyp-Mamba:\nPolyp Segmentation with Visual Mamba. In Linguraru,\nM. G.; Dou, Q"
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 9,
"chunk_index": 4,
"token_count": 140,
"text": ".; Zhou, Z.; Wu, W.; Yang, Y .;\nLiang, Y .; Jiang, J.; Cai, X.; and Su, J. 2024. Polyp-Mamba:\nPolyp Segmentation with Visual Mamba. In Linguraru,\nM. G.; Dou, Q.; Feragen, A.; Giannarou, S.; Glocker, B.;\nLekadir, K.; and Schnabel, J. A., eds., Medical Image Com-\nputing and Computer Assisted Intervention – MICCAI 2024,\n510–521. Cham: Springer Nature Switzerland. ISBN 978-\n3-031-72111-3."
},
{
"filename": "AAAI conference format (1).pdf",
"page_number": 10,
"chunk_index": 0,
"token_count": 381,
"text": "Yeung, M.; Sala, E.; Sch¨onlieb, C.-B.; and Rundo, L. 2021.\nFocus U-Net: A Novel Dual Attention-Gated CNN for Polyp\nSegmentation During Colonoscopy.\nYou, C.; Jiao, L.; Li, L.; Liu, X.; Liu, F.; Ma, W.; and Yang,\nS. 2025. Contour Knowledge-Aware Perception Learning\nfor Semantic Segmentation. IEEE Transactions on Circuits\nand Systems for Video Technology, 35(5): 4560–4575.\nZhang, M.; Yu, Y .; Gu, L.; Lin, T.; and Tao, X. 2024. VM-\nUNET-V2: Rethinking Vision Mamba UNet for Medical Im-\nage Segmentation.\nZhao, X.; Tang, F.; Wang, X.; and Xiao, J. 2024. SFC:\nShared Feature Calibration in Weakly Supervised Semantic\nSegmentation. arXiv:2401.11719.\nZhou, X.; Wu, G.; Sun, X.; Hu, P.; and Liu, Y . 2024.\nAttention-Based Multi-Kernelized and Boundary-Aware\nNetwork for Image Semantic Segmentation. Neurocomput-\ning, 597: 127988.\nZhou, Z.; Siddiquee, M. M. R.; Tajbakhsh, N.; and Liang, J.\n2018. UNet++: A Nested U-Net Architecture for Medical\nImage Segmentation. arXiv:1807.10165.\nZhu, L.; Liao, B.; Zhang, Q.; Wang, X.; Liu, W.; and Wang,\nX. 2024. Vision Mamba: Efficient Visual Representation\nLearning with Bidirectional State Space Model."
}
]